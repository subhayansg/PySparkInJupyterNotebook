{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0992018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293cc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100, 'Mariele',402.19),\n",
    "(101, 'Natka',692.77),\n",
    "(102, 'Joleen',658.17),\n",
    "(103, 'Alexine',182.05)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f21c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f0a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|100|Mariele|402.19|\n",
      "|101|  Natka|692.77|\n",
      "|102| Joleen|658.17|\n",
      "|103|Alexine|182.05|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5669df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ee42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c0e63",
   "metadata": {},
   "source": [
    "___update each id of the employee to id+1, name to uppercase, and salary to 10% increase___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92235268",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_df = raw_df. \\\n",
    "    withColumn(\"id\", col(\"id\")+1). \\\n",
    "    withColumn(\"name\", upper(col(\"name\"))). \\\n",
    "    withColumn(\"salary\", round(col(\"salary\")*110/100).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda143ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|101|MARIELE|   442|\n",
      "|102|  NATKA|   762|\n",
      "|103| JOLEEN|   724|\n",
      "|104|ALEXINE|   200|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81763496",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_df2 = raw_df. \\\n",
    "    withColumn(\"id\", col(\"id\") + lit(1)). \\\n",
    "    withColumn(\"name\", upper(col(\"name\"))). \\\n",
    "    withColumn(\"salary\", round(col(\"salary\") + (lit(0.1)*col(\"salary\"))).cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25eea7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|101|MARIELE|   442|\n",
      "|102|  NATKA|   762|\n",
      "|103| JOLEEN|   724|\n",
      "|104|ALEXINE|   200|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upd_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da811495",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeRecords = [\n",
    "  (1, \"John\",100000),\n",
    "  (2, \"Paul\",200000),\n",
    "  (3, \"Peter\",300000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676396f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(employeeRecords, \"id INT, name STRING, salary INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29466ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1| John|100000|\n",
      "|  2| Paul|200000|\n",
      "|  3|Peter|300000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f72c83",
   "metadata": {},
   "source": [
    "Given the above dataframe print \"name\" and \"salary\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8060201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| John|100000|\n",
      "| Paul|200000|\n",
      "|Peter|300000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(\"name\",\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315825d3",
   "metadata": {},
   "source": [
    "what are the other ways you can select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19c4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| John|100000|\n",
      "| Paul|200000|\n",
      "|Peter|300000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(col(\"name\"), emp_df[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801733e",
   "metadata": {},
   "source": [
    "Write a code to return dataframe having [1,2,3,4,5,6]\n",
    "\n",
    "df1 [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "df2[1,3,5,11]\n",
    "\n",
    "df3[2,4,6,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b51de0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9e6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([1,2,3,4,5,6,7,8,9,10], IntegerType())\n",
    "df2 = spark.createDataFrame([1,3,5,11], IntegerType())\n",
    "df3 = spark.createDataFrame([2,4,6,12], IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e4f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2.union(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9663d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = df4.intersect(df1).orderBy(\"value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccae74",
   "metadata": {},
   "source": [
    "### Count null values of individual columns using Spark.\n",
    "\n",
    "```Input data\n",
    "----------------------\n",
    "Col1,  col2,  col3,  col4\n",
    "1  ,   null,   che,   1000\n",
    "2  ,   mani, hyd,   null\n",
    "3  ,  Smith, null,  1200\n",
    "4  ,    ram,  Del,    3200\n",
    "5  ,  Krish,  null,   1400\n",
    "\n",
    "Output is\n",
    "---------------------\n",
    "Col1, col2, col3, col4\n",
    "0    ,   1  ,    2    ,   1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a6ca91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1 , None, \"che\", 1000),\n",
    "(2 , \"mani\", \"hyd\", None),\n",
    "(3 , \"Smith\", None, 1200),\n",
    "(4 , \"ram\", \"Del\", 3200),\n",
    "(5 , \"Krish\", None, 1400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b5b9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\", \"col4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "caba88be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+\n",
      "|col1| col2|col3|col4|\n",
      "+----+-----+----+----+\n",
      "|   1| null| che|1000|\n",
      "|   2| mani| hyd|null|\n",
      "|   3|Smith|null|1200|\n",
      "|   4|  ram| Del|3200|\n",
      "|   5|Krish|null|1400|\n",
      "+----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6dc4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = raw_df. \\\n",
    "    select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in raw_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a31572c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   0|   1|   2|   1|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd9d0833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   0|   1|   2|   1|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = raw_df. \\\n",
    "    select([count(when(col(c).isNull(), c)).alias(c) for c in raw_df.columns]). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b824726",
   "metadata": {},
   "source": [
    "### How to check if two dataframes are equal or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccbf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6575ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(123, 'ABC'),\n",
    "(123, 'DEF'),\n",
    "(123, 'XYZ'),\n",
    "(456, 'PQR'),\n",
    "(456, 'MNO'),\n",
    "(789, 'UVW')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "242a2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.createDataFrame(data, \"packid INT, MoleculeName STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e14d9aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|packid|MoleculeName|\n",
      "+------+------------+\n",
      "|   123|         ABC|\n",
      "|   123|         DEF|\n",
      "|   123|         XYZ|\n",
      "|   456|         PQR|\n",
      "|   456|         MNO|\n",
      "|   789|         UVW|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49ca7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = raw_df. \\\n",
    "    groupBy(\"packid\"). \\\n",
    "    agg(concat_ws(\"|\", collect_list(\"MoleculeName\")).alias(\"PackMoleculeString\")). \\\n",
    "    sort(\"packid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8905e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|packid|PackMoleculeString|\n",
      "+------+------------------+\n",
      "|   123|       ABC|DEF|XYZ|\n",
      "|   456|           PQR|MNO|\n",
      "|   789|               UVW|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca8c93",
   "metadata": {},
   "source": [
    "https://kontext.tech/column/spark/455/tutorial-add-constant-column-to-pyspark-dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7bfa052",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('01/01/2021', 5000),\n",
    "('02/01/2021', 3000),\n",
    "('03/01/2021', -4000),\n",
    "('04/01/2021', 6000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73af11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, \"date STRING, amount INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abf17be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|amount|\n",
      "+----------+------+\n",
      "|01/01/2021|  5000|\n",
      "|02/01/2021|  3000|\n",
      "|03/01/2021| -4000|\n",
      "|04/01/2021|  6000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cca9ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84cc7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_spec = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "res = df. \\\n",
    "    withColumn(\"amount_new\", sum(\"amount\").over(win_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "206a594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "|      date|amount|amount_new|\n",
      "+----------+------+----------+\n",
      "|01/01/2021|  5000|      5000|\n",
      "|02/01/2021|  3000|      8000|\n",
      "|03/01/2021| -4000|      4000|\n",
      "|04/01/2021|  6000|     10000|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee367a",
   "metadata": {},
   "source": [
    "### Remove first 3 lines from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ea3cfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country,city,value\n",
      "India,Pune,100\n",
      "India,Mumbai,200\n",
      "India,Pune,400\n",
      "India,Mumbai,200\n",
      "India,Pune,600\n",
      "Europe,Germany,300\n",
      "Europe,NL,100"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat new21.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52c7b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "    read. \\\n",
    "    csv(\"new21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82b32e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|    _c0|    _c1|  _c2|\n",
      "+-------+-------+-----+\n",
      "|country|   city|value|\n",
      "|  India|   Pune|  100|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  400|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  600|\n",
      "| Europe|Germany|  300|\n",
      "| Europe|     NL|  100|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2177b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"Index\", monotonically_increasing_id()). \\\n",
    "    filter('Index > 2'). \\\n",
    "    drop(\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50b6b127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|   _c0|    _c1|_c2|\n",
      "+------+-------+---+\n",
      "| India|   Pune|400|\n",
      "| India| Mumbai|200|\n",
      "| India|   Pune|600|\n",
      "|Europe|Germany|300|\n",
      "|Europe|     NL|100|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5cb8b",
   "metadata": {},
   "source": [
    "#### However the above approach can fail. It's only going to work if the first 3 rows are in the first partition. Indeed, the contract in the API is just \"The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive\". It is therefore not very safe to assume that they will always start from zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "878d1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d4b6941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>_c2</th></tr>\n",
       "<tr><td>country</td><td>city</td><td>value</td></tr>\n",
       "<tr><td>India</td><td>Pune</td><td>100</td></tr>\n",
       "<tr><td>India</td><td>Mumbai</td><td>200</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------+-----+\n",
       "|    _c0|   _c1|  _c2|\n",
       "+-------+------+-----+\n",
       "|country|  city|value|\n",
       "|  India|  Pune|  100|\n",
       "|  India|Mumbai|  200|\n",
       "+-------+------+-----+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b029a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.subtract(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adef177c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|   _c0|    _c1|_c2|\n",
      "+------+-------+---+\n",
      "|Europe|Germany|300|\n",
      "| India|   Pune|600|\n",
      "|Europe|     NL|100|\n",
      "| India|   Pune|400|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7b4a9",
   "metadata": {},
   "source": [
    "#### Using rdd to skip lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "613dd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"new21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14b1c1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country,city,value',\n",
       " 'India,Pune,100',\n",
       " 'India,Mumbai,200',\n",
       " 'India,Pune,400',\n",
       " 'India,Mumbai,200',\n",
       " 'India,Pune,600',\n",
       " 'Europe,Germany,300',\n",
       " 'Europe,NL,100']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ac1a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = rdd1.zipWithIndex(). \\\n",
    "    filter(lambda x: x[1] > 2). \\\n",
    "    map(lambda x: x[0]). \\\n",
    "    map(lambda x: x.split(\",\")). \\\n",
    "    toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "287b911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|    _1|     _2| _3|\n",
      "+------+-------+---+\n",
      "| India|   Pune|400|\n",
      "| India| Mumbai|200|\n",
      "| India|   Pune|600|\n",
      "|Europe|Germany|300|\n",
      "|Europe|     NL|100|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc207e94",
   "metadata": {},
   "source": [
    "### load dataframe, skip first 2 lines except header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f95ad88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country,city,value\n",
      "India,Pune,100\n",
      "India,Mumbai,200\n",
      "India,Pune,400\n",
      "India,Mumbai,200\n",
      "India,Pune,600\n",
      "Europe,Germany,300\n",
      "Europe,NL,100"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat new21.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "209aed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_head = spark.read.option(\"header\", \"true\").csv(\"new21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1afb9eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|country|   city|value|\n",
      "+-------+-------+-----+\n",
      "|  India|   Pune|  100|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  400|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  600|\n",
      "| Europe|Germany|  300|\n",
      "| Europe|     NL|  100|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_head.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72fe54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = df_with_head.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8aa4cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='India', city='Pune', value='100'),\n",
       " Row(country='India', city='Mumbai', value='200'),\n",
       " Row(country='India', city='Pune', value='400'),\n",
       " Row(country='India', city='Mumbai', value='200'),\n",
       " Row(country='India', city='Pune', value='600'),\n",
       " Row(country='Europe', city='Germany', value='300'),\n",
       " Row(country='Europe', city='NL', value='100')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e10ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = rdd1.zipWithIndex(). \\\n",
    "    filter(lambda x: x[1] > 1). \\\n",
    "    map(lambda x: x[0]). \\\n",
    "    toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85099afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|country|   city|value|\n",
      "+-------+-------+-----+\n",
      "|  India|   Pune|  400|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  600|\n",
      "| Europe|Germany|  300|\n",
      "| Europe|     NL|  100|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32af6d",
   "metadata": {},
   "source": [
    "### Get the last row of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c83bbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|   _c0|_c1|_c2|\n",
      "+------+---+---+\n",
      "|Europe| NL|100|\n",
      "+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expr = [last(col).alias(col) for col in df.columns]\n",
    "\n",
    "df.agg(*expr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d8df1",
   "metadata": {},
   "source": [
    "### Get Last N row from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85b81e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|   _c0|    _c1|_c2|\n",
      "+------+-------+---+\n",
      "|Europe|     NL|100|\n",
      "|Europe|Germany|300|\n",
      "| India|   Pune|600|\n",
      "| India| Mumbai|200|\n",
      "| India|   Pune|400|\n",
      "+------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "df3 = df2. \\\n",
    "    orderBy(desc(\"index\")). \\\n",
    "    drop(\"index\")\n",
    "\n",
    "# N=5\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41806a8d",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/44077404/how-to-skip-lines-while-reading-a-csv-file-as-a-dataframe-using-pyspark\n",
    "\n",
    "https://stackoverflow.com/questions/61781152/pyspark-remove-first-row-from-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fdbd4",
   "metadata": {},
   "source": [
    "### Load csv file with multiple delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d06eff76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name@|#age@|#gender\n",
      "\"Name1\"@|#34@|#Male\n",
      "\"Name2\"@|#60@|#Female\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat delim.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42316f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "    read. \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"sep\", \"@|#\"). \\\n",
    "    csv(\"delim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c41d7d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "|Name1| 34|  Male|\n",
      "|Name2| 60|Female|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11c00f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d25093ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE subhayang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2cd63624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>subhayang</td><td>alter_col_pos</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>covid_ind</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>covid_ind_ext_optm</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>cust_bucketed</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>customers</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>customers_bucketed</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>eloc</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>employee_internal</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>employees_demo</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>h</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>hbase_table_1</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>new1</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>new2</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orc_to_parquet</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>ord_dyn_prt_w_ctry</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_bucketed</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_dyn_part</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_non_part</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_stat_part</td><td>false</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+------------------+-----------+\n",
       "| database|         tableName|isTemporary|\n",
       "+---------+------------------+-----------+\n",
       "|subhayang|     alter_col_pos|      false|\n",
       "|subhayang|         covid_ind|      false|\n",
       "|subhayang|covid_ind_ext_optm|      false|\n",
       "|subhayang|     cust_bucketed|      false|\n",
       "|subhayang|         customers|      false|\n",
       "|subhayang|customers_bucketed|      false|\n",
       "|subhayang|              eloc|      false|\n",
       "|subhayang| employee_internal|      false|\n",
       "|subhayang|    employees_demo|      false|\n",
       "|subhayang|                 h|      false|\n",
       "|subhayang|     hbase_table_1|      false|\n",
       "|subhayang|              new1|      false|\n",
       "|subhayang|              new2|      false|\n",
       "|subhayang|    orc_to_parquet|      false|\n",
       "|subhayang|ord_dyn_prt_w_ctry|      false|\n",
       "|subhayang|            orders|      false|\n",
       "|subhayang|   orders_bucketed|      false|\n",
       "|subhayang|   orders_dyn_part|      false|\n",
       "|subhayang|   orders_non_part|      false|\n",
       "|subhayang|  orders_stat_part|      false|\n",
       "+---------+------------------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7588fe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_id</th><th>customer_fname</th><th>customer_lname</th><th>customer_email</th><th>customer_password</th><th>customer_street</th><th>customer_city</th><th>customer_state</th><th>customer_zipcode</th></tr>\n",
       "<tr><td>3110</td><td>Phillip</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>7983 Amber Robin ...</td><td>Irwin</td><td>PA</td><td>15642</td></tr>\n",
       "<tr><td>3111</td><td>Mary</td><td>Brown</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>8344 Sunny Embers...</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>3112</td><td>Kimberly</td><td>Marsh</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>2423 Tawny Rabbit...</td><td>Milwaukee</td><td>WI</td><td>53209</td></tr>\n",
       "<tr><td>3113</td><td>Aaron</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>9539 Rustic Beaco...</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>3114</td><td>Mary</td><td>Briggs</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>5292 Heather Close</td><td>Billings</td><td>MT</td><td>59102</td></tr>\n",
       "<tr><td>3115</td><td>Tammy</td><td>Leblanc</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>4494 Harvest Bay</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>3116</td><td>Mary</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>5841 Noble Loop</td><td>Dayton</td><td>OH</td><td>45424</td></tr>\n",
       "<tr><td>3117</td><td>Jacob</td><td>Murphy</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>3460 Middle Shado...</td><td>Rowland Heights</td><td>CA</td><td>91748</td></tr>\n",
       "<tr><td>3118</td><td>Mary</td><td>Francis</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>2544 Honey Rise Loop</td><td>Jacksonville</td><td>NC</td><td>28540</td></tr>\n",
       "<tr><td>3119</td><td>Shirley</td><td>Hinton</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>1927 Harvest Lane</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+--------------+--------------+--------------+-----------------+--------------------+---------------+--------------+----------------+\n",
       "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|  customer_city|customer_state|customer_zipcode|\n",
       "+-----------+--------------+--------------+--------------+-----------------+--------------------+---------------+--------------+----------------+\n",
       "|       3110|       Phillip|         Smith|     XXXXXXXXX|        XXXXXXXXX|7983 Amber Robin ...|          Irwin|            PA|           15642|\n",
       "|       3111|          Mary|         Brown|     XXXXXXXXX|        XXXXXXXXX|8344 Sunny Embers...|         Caguas|            PR|           00725|\n",
       "|       3112|      Kimberly|         Marsh|     XXXXXXXXX|        XXXXXXXXX|2423 Tawny Rabbit...|      Milwaukee|            WI|           53209|\n",
       "|       3113|         Aaron|         Smith|     XXXXXXXXX|        XXXXXXXXX|9539 Rustic Beaco...|         Caguas|            PR|           00725|\n",
       "|       3114|          Mary|        Briggs|     XXXXXXXXX|        XXXXXXXXX|  5292 Heather Close|       Billings|            MT|           59102|\n",
       "|       3115|         Tammy|       Leblanc|     XXXXXXXXX|        XXXXXXXXX|    4494 Harvest Bay|         Caguas|            PR|           00725|\n",
       "|       3116|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|     5841 Noble Loop|         Dayton|            OH|           45424|\n",
       "|       3117|         Jacob|        Murphy|     XXXXXXXXX|        XXXXXXXXX|3460 Middle Shado...|Rowland Heights|            CA|           91748|\n",
       "|       3118|          Mary|       Francis|     XXXXXXXXX|        XXXXXXXXX|2544 Honey Rise Loop|   Jacksonville|            NC|           28540|\n",
       "|       3119|       Shirley|        Hinton|     XXXXXXXXX|        XXXXXXXXX|   1927 Harvest Lane|         Caguas|            PR|           00725|\n",
       "+-----------+--------------+--------------+--------------+-----------------+--------------------+---------------+--------------+----------------+"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from customers limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00bfda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
