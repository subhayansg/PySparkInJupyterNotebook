{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648b09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313763d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100, 'Mariele',402.19),\n",
    "(101, 'Natka',692.77),\n",
    "(102, 'Joleen',658.17),\n",
    "(103, 'Alexine',182.05)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b34b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5312494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|100|Mariele|402.19|\n",
      "|101|  Natka|692.77|\n",
      "|102| Joleen|658.17|\n",
      "|103|Alexine|182.05|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a62d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10cf5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef33b3",
   "metadata": {},
   "source": [
    "___update each id of the employee to id+1, name to uppercase, and salary to 10% increase___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b398d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_df = raw_df. \\\n",
    "    withColumn(\"id\", col(\"id\")+1). \\\n",
    "    withColumn(\"name\", upper(col(\"name\"))). \\\n",
    "    withColumn(\"salary\", round(col(\"salary\")*110/100).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37241257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|101|MARIELE|   442|\n",
      "|102|  NATKA|   762|\n",
      "|103| JOLEEN|   724|\n",
      "|104|ALEXINE|   200|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df9e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_df2 = raw_df. \\\n",
    "    withColumn(\"id\", col(\"id\") + lit(1)). \\\n",
    "    withColumn(\"name\", upper(col(\"name\"))). \\\n",
    "    withColumn(\"salary\", round(col(\"salary\") + (lit(0.1)*col(\"salary\"))).cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f336bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|101|MARIELE|   442|\n",
      "|102|  NATKA|   762|\n",
      "|103| JOLEEN|   724|\n",
      "|104|ALEXINE|   200|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upd_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f266fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeRecords = [\n",
    "  (1, \"John\",100000),\n",
    "  (2, \"Paul\",200000),\n",
    "  (3, \"Peter\",300000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192eff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(employeeRecords, \"id INT, name STRING, salary INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ce6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1| John|100000|\n",
      "|  2| Paul|200000|\n",
      "|  3|Peter|300000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc937ba",
   "metadata": {},
   "source": [
    "Given the above dataframe print \"name\" and \"salary\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace4d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| John|100000|\n",
      "| Paul|200000|\n",
      "|Peter|300000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(\"name\",\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f95d8d",
   "metadata": {},
   "source": [
    "what are the other ways you can select "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a672bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "| John|100000|\n",
      "| Paul|200000|\n",
      "|Peter|300000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(col(\"name\"), emp_df[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce99de",
   "metadata": {},
   "source": [
    "Write a code to return dataframe having [1,2,3,4,5,6]\n",
    "\n",
    "df1 [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "df2[1,3,5,11]\n",
    "\n",
    "df3[2,4,6,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58306ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb32decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([1,2,3,4,5,6,7,8,9,10], IntegerType())\n",
    "df2 = spark.createDataFrame([1,3,5,11], IntegerType())\n",
    "df3 = spark.createDataFrame([2,4,6,12], IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39fd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2.union(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32078cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "|    6|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = df4.intersect(df1).orderBy(\"value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87801047",
   "metadata": {},
   "source": [
    "### Count null values of individual columns using Spark.\n",
    "\n",
    "```Input data\n",
    "----------------------\n",
    "Col1,  col2,  col3,  col4\n",
    "1  ,   null,   che,   1000\n",
    "2  ,   mani, hyd,   null\n",
    "3  ,  Smith, null,  1200\n",
    "4  ,    ram,  Del,    3200\n",
    "5  ,  Krish,  null,   1400\n",
    "\n",
    "Output is\n",
    "---------------------\n",
    "Col1, col2, col3, col4\n",
    "0    ,   1  ,    2    ,   1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a52baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1 , None, \"che\", 1000),\n",
    "(2 , \"mani\", \"hyd\", None),\n",
    "(3 , \"Smith\", None, 1200),\n",
    "(4 , \"ram\", \"Del\", 3200),\n",
    "(5 , \"Krish\", None, 1400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba84b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.createDataFrame(data, [\"col1\", \"col2\", \"col3\", \"col4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4ede68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+\n",
      "|col1| col2|col3|col4|\n",
      "+----+-----+----+----+\n",
      "|   1| null| che|1000|\n",
      "|   2| mani| hyd|null|\n",
      "|   3|Smith|null|1200|\n",
      "|   4|  ram| Del|3200|\n",
      "|   5|Krish|null|1400|\n",
      "+----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce114ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = raw_df. \\\n",
    "    select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in raw_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dcf5c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   0|   1|   2|   1|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee8f5de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   0|   1|   2|   1|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res2 = raw_df. \\\n",
    "    select([count(when(col(c).isNull(), c)).alias(c) for c in raw_df.columns]). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5dcc6",
   "metadata": {},
   "source": [
    "### How to check if two dataframes are equal or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5a21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3028de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(123, 'ABC'),\n",
    "(123, 'DEF'),\n",
    "(123, 'XYZ'),\n",
    "(456, 'PQR'),\n",
    "(456, 'MNO'),\n",
    "(789, 'UVW')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c716e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.createDataFrame(data, \"packid INT, MoleculeName STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0fcfe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|packid|MoleculeName|\n",
      "+------+------------+\n",
      "|   123|         ABC|\n",
      "|   123|         DEF|\n",
      "|   123|         XYZ|\n",
      "|   456|         PQR|\n",
      "|   456|         MNO|\n",
      "|   789|         UVW|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba2f2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = raw_df. \\\n",
    "    groupBy(\"packid\"). \\\n",
    "    agg(concat_ws(\"|\", collect_list(\"MoleculeName\")).alias(\"PackMoleculeString\")). \\\n",
    "    sort(\"packid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f635e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|packid|PackMoleculeString|\n",
      "+------+------------------+\n",
      "|   123|       ABC|DEF|XYZ|\n",
      "|   456|           PQR|MNO|\n",
      "|   789|               UVW|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44765ff2",
   "metadata": {},
   "source": [
    "https://kontext.tech/column/spark/455/tutorial-add-constant-column-to-pyspark-dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe9d11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('01/01/2021', 5000),\n",
    "('02/01/2021', 3000),\n",
    "('03/01/2021', -4000),\n",
    "('04/01/2021', 6000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6179c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, \"date STRING, amount INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95298a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date|amount|\n",
      "+----------+------+\n",
      "|01/01/2021|  5000|\n",
      "|02/01/2021|  3000|\n",
      "|03/01/2021| -4000|\n",
      "|04/01/2021|  6000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65f25884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "922c9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_spec = Window. \\\n",
    "    orderBy(\"date\"). \\\n",
    "    rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "res = df. \\\n",
    "    withColumn(\"amount_new\", sum(\"amount\").over(win_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd7535ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+\n",
      "|      date|amount|amount_new|\n",
      "+----------+------+----------+\n",
      "|01/01/2021|  5000|      5000|\n",
      "|02/01/2021|  3000|      8000|\n",
      "|03/01/2021| -4000|      4000|\n",
      "|04/01/2021|  6000|     10000|\n",
      "+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258af10",
   "metadata": {},
   "source": [
    "### Remove first 3 lines from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9fc4c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country,city,value\n",
      "India,Pune,100\n",
      "India,Mumbai,200\n",
      "India,Pune,400\n",
      "India,Mumbai,200\n",
      "India,Pune,600\n",
      "Europe,Germany,300\n",
      "Europe,NL,100"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat new21.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c5cfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "    read. \\\n",
    "    csv(\"new21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a118610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|    _c0|    _c1|  _c2|\n",
      "+-------+-------+-----+\n",
      "|country|   city|value|\n",
      "|  India|   Pune|  100|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  400|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  600|\n",
      "| Europe|Germany|  300|\n",
      "| Europe|     NL|  100|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6a08cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"Index\", monotonically_increasing_id()). \\\n",
    "    filter('Index > 2'). \\\n",
    "    drop(\"Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2d2fe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|   _c0|    _c1|_c2|\n",
      "+------+-------+---+\n",
      "| India|   Pune|400|\n",
      "| India| Mumbai|200|\n",
      "| India|   Pune|600|\n",
      "|Europe|Germany|300|\n",
      "|Europe|     NL|100|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab6036",
   "metadata": {},
   "source": [
    "#### However the above approach can fail. It's only going to work if the first 3 rows are in the first partition. Indeed, the contract in the API is just \"The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive\". It is therefore not very safe to assume that they will always start from zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd7ba970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25e79691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_c0</th><th>_c1</th><th>_c2</th></tr>\n",
       "<tr><td>country</td><td>city</td><td>value</td></tr>\n",
       "<tr><td>India</td><td>Pune</td><td>100</td></tr>\n",
       "<tr><td>India</td><td>Mumbai</td><td>200</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------+-----+\n",
       "|    _c0|   _c1|  _c2|\n",
       "+-------+------+-----+\n",
       "|country|  city|value|\n",
       "|  India|  Pune|  100|\n",
       "|  India|Mumbai|  200|\n",
       "+-------+------+-----+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b955c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.subtract(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12c7156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|   _c0|    _c1|_c2|\n",
      "+------+-------+---+\n",
      "|Europe|Germany|300|\n",
      "| India|   Pune|600|\n",
      "|Europe|     NL|100|\n",
      "| India|   Pune|400|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b35f7",
   "metadata": {},
   "source": [
    "#### Using rdd to skip lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "464294be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"new21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5628a948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country,city,value',\n",
       " 'India,Pune,100',\n",
       " 'India,Mumbai,200',\n",
       " 'India,Pune,400',\n",
       " 'India,Mumbai,200',\n",
       " 'India,Pune,600',\n",
       " 'Europe,Germany,300',\n",
       " 'Europe,NL,100']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0408264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = rdd1.zipWithIndex(). \\\n",
    "    filter(lambda x: x[1] > 2). \\\n",
    "    map(lambda x: x[0]). \\\n",
    "    map(lambda x: x.split(\",\")). \\\n",
    "    toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b346375e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|    _1|     _2| _3|\n",
      "+------+-------+---+\n",
      "| India|   Pune|400|\n",
      "| India| Mumbai|200|\n",
      "| India|   Pune|600|\n",
      "|Europe|Germany|300|\n",
      "|Europe|     NL|100|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699d342",
   "metadata": {},
   "source": [
    "### load dataframe, skip first 2 lines except header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b875ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country,city,value\n",
      "India,Pune,100\n",
      "India,Mumbai,200\n",
      "India,Pune,400\n",
      "India,Mumbai,200\n",
      "India,Pune,600\n",
      "Europe,Germany,300\n",
      "Europe,NL,100"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat new21.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "591cfe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_head = spark.read.option(\"header\", \"true\").csv(\"new21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a09ec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|country|   city|value|\n",
      "+-------+-------+-----+\n",
      "|  India|   Pune|  100|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  400|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  600|\n",
      "| Europe|Germany|  300|\n",
      "| Europe|     NL|  100|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_head.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2affc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = df_with_head.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52f45f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='India', city='Pune', value='100'),\n",
       " Row(country='India', city='Mumbai', value='200'),\n",
       " Row(country='India', city='Pune', value='400'),\n",
       " Row(country='India', city='Mumbai', value='200'),\n",
       " Row(country='India', city='Pune', value='600'),\n",
       " Row(country='Europe', city='Germany', value='300'),\n",
       " Row(country='Europe', city='NL', value='100')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c0d6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_df = rdd1.zipWithIndex(). \\\n",
    "    filter(lambda x: x[1] > 1). \\\n",
    "    map(lambda x: x[0]). \\\n",
    "    toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "203e46b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|country|   city|value|\n",
      "+-------+-------+-----+\n",
      "|  India|   Pune|  400|\n",
      "|  India| Mumbai|  200|\n",
      "|  India|   Pune|  600|\n",
      "| Europe|Germany|  300|\n",
      "| Europe|     NL|  100|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d62ce2",
   "metadata": {},
   "source": [
    "### Get the last row of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5f21bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+\n",
      "|   _c0|_c1|_c2|\n",
      "+------+---+---+\n",
      "|Europe| NL|100|\n",
      "+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expr = [last(col).alias(col) for col in df.columns]\n",
    "\n",
    "df.agg(*expr).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c333e5a",
   "metadata": {},
   "source": [
    "### Get Last N row from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f3ff5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|   _c0|    _c1|_c2|\n",
      "+------+-------+---+\n",
      "|Europe|     NL|100|\n",
      "|Europe|Germany|300|\n",
      "| India|   Pune|600|\n",
      "| India| Mumbai|200|\n",
      "| India|   Pune|400|\n",
      "+------+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "df3 = df2. \\\n",
    "    orderBy(desc(\"index\")). \\\n",
    "    drop(\"index\")\n",
    "\n",
    "# N=5\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2199d",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/44077404/how-to-skip-lines-while-reading-a-csv-file-as-a-dataframe-using-pyspark\n",
    "\n",
    "https://stackoverflow.com/questions/61781152/pyspark-remove-first-row-from-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e14a7",
   "metadata": {},
   "source": [
    "### Load csv file with multiple delimiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f58c26e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name@|#age@|#gender\n",
      "\"Name1\"@|#34@|#Male\n",
      "\"Name2\"@|#60@|#Female\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat delim.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd660053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark. \\\n",
    "    read. \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"sep\", \"@|#\"). \\\n",
    "    csv(\"delim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6605d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|gender|\n",
      "+-----+---+------+\n",
      "|Name1| 34|  Male|\n",
      "|Name2| 60|Female|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12be0d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa1ff391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE subhayang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cbd4db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>database</th><th>tableName</th><th>isTemporary</th></tr>\n",
       "<tr><td>subhayang</td><td>alter_col_pos</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>covid_ind</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>covid_ind_ext_optm</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>cust_bucketed</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>customers</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>customers_bucketed</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>eloc</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>employee_internal</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>employees_demo</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>h</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>hbase_table_1</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>new1</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>new2</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orc_to_parquet</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>ord_dyn_prt_w_ctry</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_bucketed</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_dyn_part</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_non_part</td><td>false</td></tr>\n",
       "<tr><td>subhayang</td><td>orders_stat_part</td><td>false</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------+------------------+-----------+\n",
       "| database|         tableName|isTemporary|\n",
       "+---------+------------------+-----------+\n",
       "|subhayang|     alter_col_pos|      false|\n",
       "|subhayang|         covid_ind|      false|\n",
       "|subhayang|covid_ind_ext_optm|      false|\n",
       "|subhayang|     cust_bucketed|      false|\n",
       "|subhayang|         customers|      false|\n",
       "|subhayang|customers_bucketed|      false|\n",
       "|subhayang|              eloc|      false|\n",
       "|subhayang| employee_internal|      false|\n",
       "|subhayang|    employees_demo|      false|\n",
       "|subhayang|                 h|      false|\n",
       "|subhayang|     hbase_table_1|      false|\n",
       "|subhayang|              new1|      false|\n",
       "|subhayang|              new2|      false|\n",
       "|subhayang|    orc_to_parquet|      false|\n",
       "|subhayang|ord_dyn_prt_w_ctry|      false|\n",
       "|subhayang|            orders|      false|\n",
       "|subhayang|   orders_bucketed|      false|\n",
       "|subhayang|   orders_dyn_part|      false|\n",
       "|subhayang|   orders_non_part|      false|\n",
       "|subhayang|  orders_stat_part|      false|\n",
       "+---------+------------------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "042cfcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer_id</th><th>customer_fname</th><th>customer_lname</th><th>customer_email</th><th>customer_password</th><th>customer_street</th><th>customer_city</th><th>customer_state</th><th>customer_zipcode</th></tr>\n",
       "<tr><td>1</td><td>Richard</td><td>Hernandez</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>6303 Heather Plaza</td><td>Brownsville</td><td>TX</td><td>78521</td></tr>\n",
       "<tr><td>2</td><td>Mary</td><td>Barrett</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>9526 Noble Embers...</td><td>Littleton</td><td>CO</td><td>80126</td></tr>\n",
       "<tr><td>3</td><td>Ann</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>3422 Blue Pioneer...</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>4</td><td>Mary</td><td>Jones</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>8324 Little Common</td><td>San Marcos</td><td>CA</td><td>92069</td></tr>\n",
       "<tr><td>5</td><td>Robert</td><td>Hudson</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>10 Crystal River ...</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>6</td><td>Mary</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>3151 Sleepy Quail...</td><td>Passaic</td><td>NJ</td><td>07055</td></tr>\n",
       "<tr><td>7</td><td>Melissa</td><td>Wilcox</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>9453 High Concession</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>8</td><td>Megan</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>3047 Foggy Forest...</td><td>Lawrence</td><td>MA</td><td>01841</td></tr>\n",
       "<tr><td>9</td><td>Mary</td><td>Perez</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>3616 Quaking Street</td><td>Caguas</td><td>PR</td><td>00725</td></tr>\n",
       "<tr><td>10</td><td>Melissa</td><td>Smith</td><td>XXXXXXXXX</td><td>XXXXXXXXX</td><td>8598 Harvest Beac...</td><td>Stafford</td><td>VA</td><td>22554</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
       "|customer_id|customer_fname|customer_lname|customer_email|customer_password|     customer_street|customer_city|customer_state|customer_zipcode|\n",
       "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+\n",
       "|          1|       Richard|     Hernandez|     XXXXXXXXX|        XXXXXXXXX|  6303 Heather Plaza|  Brownsville|            TX|           78521|\n",
       "|          2|          Mary|       Barrett|     XXXXXXXXX|        XXXXXXXXX|9526 Noble Embers...|    Littleton|            CO|           80126|\n",
       "|          3|           Ann|         Smith|     XXXXXXXXX|        XXXXXXXXX|3422 Blue Pioneer...|       Caguas|            PR|           00725|\n",
       "|          4|          Mary|         Jones|     XXXXXXXXX|        XXXXXXXXX|  8324 Little Common|   San Marcos|            CA|           92069|\n",
       "|          5|        Robert|        Hudson|     XXXXXXXXX|        XXXXXXXXX|10 Crystal River ...|       Caguas|            PR|           00725|\n",
       "|          6|          Mary|         Smith|     XXXXXXXXX|        XXXXXXXXX|3151 Sleepy Quail...|      Passaic|            NJ|           07055|\n",
       "|          7|       Melissa|        Wilcox|     XXXXXXXXX|        XXXXXXXXX|9453 High Concession|       Caguas|            PR|           00725|\n",
       "|          8|         Megan|         Smith|     XXXXXXXXX|        XXXXXXXXX|3047 Foggy Forest...|     Lawrence|            MA|           01841|\n",
       "|          9|          Mary|         Perez|     XXXXXXXXX|        XXXXXXXXX| 3616 Quaking Street|       Caguas|            PR|           00725|\n",
       "|         10|       Melissa|         Smith|     XXXXXXXXX|        XXXXXXXXX|8598 Harvest Beac...|     Stafford|            VA|           22554|\n",
       "+-----------+--------------+--------------+--------------+-----------------+--------------------+-------------+--------------+----------------+"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from customers limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6f52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
