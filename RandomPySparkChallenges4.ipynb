{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126068c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('A', 1),\n",
    "       ('B', 3),\n",
    "       ('C', 4),\n",
    "       ('D', 5),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64dc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"COLA\", \"COLB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7c3ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|COLA|COLB|\n",
      "+----+----+\n",
      "|   A|   1|\n",
      "|   B|   3|\n",
      "|   C|   4|\n",
      "|   D|   5|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbe8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae3605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"add above\", sum(col(\"COLB\")).over(Window.rowsBetween(Window.unboundedPreceding, Window.currentRow))).orderBy(\"COLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc48b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|COLA|COLB|add above|\n",
      "+----+----+---------+\n",
      "|   A|   1|       10|\n",
      "|   B|   3|       13|\n",
      "|   C|   4|        4|\n",
      "|   D|   5|        9|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c10b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, '27+06+2021'),\n",
    "(2, '27+07+2021'),\n",
    "(3, '25+08+2020')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc9b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"COLA\", \"COLB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a38dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|COLA|      COLB|\n",
      "+----+----------+\n",
      "|   1|27+06+2021|\n",
      "|   2|27+07+2021|\n",
      "|   3|25+08+2020|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc58504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----------------+\n",
      "|COLA|      COLB|id_is_odd_or_even|\n",
      "+----+----------+-----------------+\n",
      "|   1|27+06+2021|              ODD|\n",
      "|   2|27+07+2021|             EVEN|\n",
      "|   3|25+08+2020|              ODD|\n",
      "+----+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"id_is_odd_or_even\", when(col(\"COLA\")% 2 == 0, \"EVEN\").otherwise(\"ODD\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a32d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+------------------+------------+-----------------+\n",
      "|COLA|      COLB|actual_date_format|day_of_week_string|week_of_year|custom_date_fomat|\n",
      "+----+----------+------------------+------------------+------------+-----------------+\n",
      "|   1|27+06+2021|        2021-06-27|               Sun|          25|2021-06-27 Sun 25|\n",
      "|   2|27+07+2021|        2021-07-27|               Tue|          30|2021-07-27 Tue 30|\n",
      "|   3|25+08+2020|        2020-08-25|               Tue|          35|2020-08-25 Tue 35|\n",
      "+----+----------+------------------+------------------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.withColumn(\"actual_date_format\", to_date(col(\"COLB\"), \"dd+MM+yyyy\")). \\\n",
    "    withColumn(\"day_of_week_string\", date_format(\"actual_date_format\", \"E\")). \\\n",
    "    withColumn(\"week_of_year\", weekofyear(\"actual_date_format\")). \\\n",
    "    withColumn(\"custom_date_fomat\", concat(\"actual_date_format\", lit(\" \"), \"day_of_week_string\", lit(\" \"), \"week_of_year\"))\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204e5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'A'),\n",
    "       (2, 'B'),\n",
    "       (3, 'C'),\n",
    "       (4, 'D')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d27a6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"COLA\", \"COLB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df3f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|COLA|COLB|\n",
      "+----+----+\n",
      "|   1|   A|\n",
      "|   2|   B|\n",
      "|   3|   C|\n",
      "|   4|   D|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d306c419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|COLA|COLB|COLC|\n",
      "+----+----+----+\n",
      "|   1|   A|   A|\n",
      "|   2|   B|  BB|\n",
      "|   3|   C| CCC|\n",
      "|   4|   D|DDDD|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"COLC\", expr(\"repeat(COLB, COLA)\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d69892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruits_list,store\n",
      "apple banana mango,more\n",
      "mango,reliance\n",
      "grapes papaya,smart\n",
      "angeer mango,kk\n",
      "orange,gr stores\n",
      "orange mango,heritage\n",
      "watermelon mango,Walmart\n",
      "musk melon,amazon\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat new23.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfff5f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruits_list,store',\n",
       " 'apple banana mango,more',\n",
       " 'mango,reliance',\n",
       " 'grapes papaya,smart',\n",
       " 'angeer mango,kk',\n",
       " 'orange,gr stores',\n",
       " 'orange mango,heritage',\n",
       " 'watermelon mango,Walmart',\n",
       " 'musk melon,amazon']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.textFile(\"/user/itv736079/new23.csv\")\n",
    "\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0f5c157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['more'], ['reliance'], [], ['kk'], [], ['heritage'], ['Walmart'], []]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x: x.split(',')). \\\n",
    "    map(lambda x: [x[1] for i in x[0].split(' ') if 'mango' in i])\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d49fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,8\n",
      "4,4\n",
      "6,5\n",
      "5,6\n",
      "3,7\n",
      "9,3\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat file1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff52340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,8\n",
      "4,4\n",
      "6,5\n",
      "5,6\n",
      "3,7\n",
      "9,3\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat file2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79fdbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/user/itv736079/file1.csv\", \"a INT, b INT\")\n",
    "df2 = spark.read.csv(\"/user/itv736079/file2.csv\", \"a INT, b INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "942325ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "722ff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df1.withColumn(\"add1\", (df1.a+df1.b).cast(IntegerType())). \\\n",
    "    withColumn(\"new\", monotonically_increasing_id())\n",
    "\n",
    "df4 = df2.withColumn(\"add2\", (df2.a+df2.b).cast(IntegerType())). \\\n",
    "    withColumn(\"new\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0dbaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df3.join(df4, df3.new == df4.new, \"inner\"). \\\n",
    "    withColumn(\"add\", (col(\"add1\")+col(\"add2\")).cast(IntegerType())). \\\n",
    "    drop(\"a\", \"b\", \"add1\", \"add2\", \"new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c2f59d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|add|\n",
      "+---+\n",
      "| 20|\n",
      "| 16|\n",
      "| 22|\n",
      "| 22|\n",
      "| 20|\n",
      "| 24|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d912b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('05/01/2021', 'A', 400),\n",
    "('15/01/2021', 'A', 300),\n",
    "('06/01/2021', 'A', 700),\n",
    "('10/01/2021', 'A', 100),\n",
    "('12/01/2021', 'B', 300),\n",
    "('14/01/2021', 'B', 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe4bfbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"date\", \"common_col\", \"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20814761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
