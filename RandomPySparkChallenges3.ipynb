{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a355667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a285c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [('ABC', 'DEF', 'GHI'),\n",
    "('PQR', 'STU', 'VWZ'),\n",
    "('SMT', 'YUH', 'SGR'),\n",
    "('SWI', 'FYG', 'LKU')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9cf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [('HI', 'HELLO', 'HOW'),\n",
    "('ARE', 'YOU', 'FINE'),\n",
    "('ETC', 'NO', 'WORRY'),\n",
    "('SAY', 'YOU', 'ARE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da883305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|ABC|DEF|GHI|\n",
      "|PQR|STU|VWZ|\n",
      "|SMT|YUH|SGR|\n",
      "|SWI|FYG|LKU|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data1, [\"A\", \"B\", \"C\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84bb09e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| A1|   B1|   C1|\n",
      "+---+-----+-----+\n",
      "| HI|HELLO|  HOW|\n",
      "|ARE|  YOU| FINE|\n",
      "|ETC|   NO|WORRY|\n",
      "|SAY|  YOU|  ARE|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame(data2, [\"A1\", \"B1\", \"C1\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b03503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|  A|    B|    C|\n",
      "+---+-----+-----+\n",
      "|ABC|  DEF|  GHI|\n",
      "|PQR|  STU|  VWZ|\n",
      "|SMT|  YUH|  SGR|\n",
      "|SWI|  FYG|  LKU|\n",
      "| HI|HELLO|  HOW|\n",
      "|ARE|  YOU| FINE|\n",
      "|ETC|   NO|WORRY|\n",
      "|SAY|  YOU|  ARE|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.union(df2)\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa6c98",
   "metadata": {},
   "source": [
    "***************************************\n",
    "Input:\n",
    "```\n",
    "+---+----+\n",
    "| id|name|\n",
    "+---+----+\n",
    "|  1|   a|\n",
    "|  1|   b|\n",
    "|  1|   c|\n",
    "|  1|   d|\n",
    "|  2|   e|\n",
    "|  2|   f|\n",
    "|  2|   g|\n",
    "+---+----+\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "+---+-----+\n",
    "| id|names|\n",
    "+---+-----+\n",
    "|  1| abcd|\n",
    "|  2|  efg|\n",
    "+---+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ecc4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'a'),\n",
    "(1,'b'),\n",
    "(1,'c'),\n",
    "(1,'d'),\n",
    "(2,'e'),\n",
    "(2,'f'),\n",
    "(2,'g')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e0a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  1|   b|\n",
      "|  1|   c|\n",
      "|  1|   d|\n",
      "|  2|   e|\n",
      "|  2|   f|\n",
      "|  2|   g|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63e323dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb15609",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupBy(\"id\"). \\\n",
    "    agg(collect_list(\"name\").alias(\"names\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57101420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|       names|\n",
      "+---+------------+\n",
      "|  1|[a, b, c, d]|\n",
      "|  2|   [e, f, g]|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e294a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.groupBy(\"id\"). \\\n",
    "    agg(concat_ws(\"\", collect_list(\"name\")).alias(\"names\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb091a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|names|\n",
      "+---+-----+\n",
      "|  1| dabc|\n",
      "|  2|  efg|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6f64db",
   "metadata": {},
   "source": [
    "***************************************\n",
    "Input:\n",
    "```\n",
    "+-----+-----+-------+\n",
    "|empno|clmno| status|\n",
    "+-----+-----+-------+\n",
    "|  101| clm1|   null|\n",
    "|  101| clm2|pending|\n",
    "|  102| clm3| delete|\n",
    "|  102| clm4|pending|\n",
    "|  103| clm5|pending|\n",
    "+-----+-----+-------+\n",
    "```\n",
    "Whenever I have null for a empid I need to get only that null record,\n",
    "Else i need to get all records\n",
    "```\n",
    "+-----+-----+-------+\n",
    "|empno|clmno| status|\n",
    "+-----+-----+-------+\n",
    "|  103| clm5|pending|\n",
    "|  101| clm1|   null|\n",
    "|  102| clm3| delete|\n",
    "|  102| clm4|pending|\n",
    "+-----+-----+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd50dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  spark.createDataFrame([(101,\"clm1\",None),(101,\"clm2\",\"pending\"),(102,\"clm3\",\"delete\"),(102,\"clm4\",\"pending\"),(103,\"clm5\",\"pending\")],[\"empno\",\"clmno\",\"status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23b206c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|empno|clmno| status|\n",
      "+-----+-----+-------+\n",
      "|  101| clm1|   null|\n",
      "|  101| clm2|pending|\n",
      "|  102| clm3| delete|\n",
      "|  102| clm4|pending|\n",
      "|  103| clm5|pending|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5cf9509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|empno|clmno| status|\n",
      "+-----+-----+-------+\n",
      "|  101| clm1|     NA|\n",
      "|  101| clm2|pending|\n",
      "|  102| clm3| delete|\n",
      "|  102| clm4|pending|\n",
      "|  103| clm5|pending|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"status\", when(col(\"status\").isNull(), \"NA\").otherwise(col(\"status\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c45a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----------------+\n",
      "|empno|clmno| status|        array_col|\n",
      "+-----+-----+-------+-----------------+\n",
      "|  103| clm5|pending|        [pending]|\n",
      "|  101| clm1|     NA|    [NA, pending]|\n",
      "|  101| clm2|pending|    [NA, pending]|\n",
      "|  102| clm3| delete|[delete, pending]|\n",
      "|  102| clm4|pending|[delete, pending]|\n",
      "+-----+-----+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_w = Window.partitionBy(\"empno\").orderBy(\"empno\")\n",
    "df = df.withColumn(\"array_col\", collect_list(col(\"status\")).over(_w))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e84e0f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----------------+----------+\n",
      "|empno|clmno| status|        array_col|filter_col|\n",
      "+-----+-----+-------+-----------------+----------+\n",
      "|  103| clm5|pending|        [pending]|     false|\n",
      "|  101| clm1|     NA|    [NA, pending]|      true|\n",
      "|  101| clm2|pending|    [NA, pending]|      true|\n",
      "|  102| clm3| delete|[delete, pending]|     false|\n",
      "|  102| clm4|pending|[delete, pending]|     false|\n",
      "+-----+-----+-------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"filter_col\", array_contains(col(\"array_col\"),\"NA\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e086e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter((col(\"filter_col\") == False) | (col(\"filter_col\") == True) & (col(\"status\") == \"NA\")). \\\n",
    "    withColumn(\"status\", when(col(\"status\") == \"NA\", None).otherwise(col(\"status\"))). \\\n",
    "    drop(\"array_col\", \"filter_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a53001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|empno|clmno| status|\n",
      "+-----+-----+-------+\n",
      "|  103| clm5|pending|\n",
      "|  101| clm1|   null|\n",
      "|  102| clm3| delete|\n",
      "|  102| clm4|pending|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
