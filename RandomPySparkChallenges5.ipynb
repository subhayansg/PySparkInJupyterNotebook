{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda27024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Basic Transformations'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f7ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd3784f",
   "metadata": {},
   "source": [
    "___How to get second highest salary department wise without using window functions and with out using native sql(spark.sql(...)) .Get the result through dataframe in spark.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e3dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Engg\", \"Sam\", 1000),\n",
    "(\"Engg\", \"Smith\", 2000),\n",
    "(\"HR\", \"Denis\", 1500),\n",
    "(\"HR\", \"Danny\", 3000),\n",
    "(\"IT\", \"David\", 2000),\n",
    "(\"IT\", \"John\", 3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13b1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_df = spark.createDataFrame(data, [\"DeptID\", \"EmpName\", \"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753b6434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|DeptID|EmpName|Salary|\n",
      "+------+-------+------+\n",
      "|  Engg|    Sam|  1000|\n",
      "|  Engg|  Smith|  2000|\n",
      "|    HR|  Denis|  1500|\n",
      "|    HR|  Danny|  3000|\n",
      "|    IT|  David|  2000|\n",
      "|    IT|   John|  3000|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sal_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34af47",
   "metadata": {},
   "source": [
    "___Using window function___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c78a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_spec = Window. \\\n",
    "    partitionBy(\"DeptID\"). \\\n",
    "    orderBy(col(\"Salary\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d3121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_highest_sal = sal_df. \\\n",
    "    withColumn(\"rank\", dense_rank().over(win_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e76c9910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----+\n",
      "|DeptID|EmpName|Salary|rank|\n",
      "+------+-------+------+----+\n",
      "|    HR|  Danny|  3000|   1|\n",
      "|    HR|  Denis|  1500|   2|\n",
      "|  Engg|  Smith|  2000|   1|\n",
      "|  Engg|    Sam|  1000|   2|\n",
      "|    IT|   John|  3000|   1|\n",
      "|    IT|  David|  2000|   2|\n",
      "+------+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_highest_sal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "875b5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_highest_sal = second_highest_sal. \\\n",
    "    filter(\"rank == 2\"). \\\n",
    "    drop(\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce89adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|DeptID|EmpName|Salary|\n",
      "+------+-------+------+\n",
      "|    HR|  Denis|  1500|\n",
      "|  Engg|    Sam|  1000|\n",
      "|    IT|  David|  2000|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_highest_sal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58574be6",
   "metadata": {},
   "source": [
    "___Without window function___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676dceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal2 = sal_df. \\\n",
    "    groupBy(\"DeptID\"). \\\n",
    "    max(\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa241d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|DeptID|max(Salary)|\n",
      "+------+-----------+\n",
      "|    HR|       3000|\n",
      "|  Engg|       2000|\n",
      "|    IT|       3000|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sal2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69d542b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[('A', 1)], [('A', 1), ('A', 1) ]]\n",
    "\n",
    "# o/p: = [[('A', 1)], [('A', 2) ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88993d11",
   "metadata": {},
   "source": [
    "1. Read a student csv file\n",
    "2. add new column name Remarks -pass/fail . It will be decided based on marks\n",
    "3. Save this data into hive table\n",
    "\n",
    "```\n",
    "df = spark.read.csv()\n",
    "df2 = df.withColumn(\"Remarks\", when(df.\"marks\" > 40, \"pass\").otherwise(\"fail\"))\n",
    "\n",
    "df2.write. \\\n",
    "\t.format(\"csv\")\n",
    "\t.mode(\"Overwrite\")\n",
    "\t.saveAsTable(\"students\")    \n",
    "```\n",
    "====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a2748",
   "metadata": {},
   "source": [
    "1)if we have duplicate column entry in data will we be able to read it using spark\n",
    "\n",
    "2)if yes is there any way i can rename such columns like\n",
    "\n",
    "```\n",
    "a b a c a b\n",
    "1 2 1 2 1 2\n",
    "```\n",
    "to\n",
    "```\n",
    "a b a_2 c a_3 b_2\n",
    "1 2 1   2 1   2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4585a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b a c a b\n",
      "1 2 1 2 1 2\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat dup_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0756163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"delimiter\", \" \").csv(\"dup_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a890195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| a0| b1| a2|  c| a4| b5|\n",
      "+---+---+---+---+---+---+\n",
      "|  1|  2|  1|  2|  1|  2|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73efe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
