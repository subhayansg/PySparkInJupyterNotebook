{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dad0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6577afac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp id|emp name|emp salary|\n",
      "+------+--------+----------+\n",
      "|    11|  Deepak|       100|\n",
      "|    12|   Leena|      null|\n",
      "|    13|   Deepa|       102|\n",
      "|    15|  Preety|      null|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nullDF = spark.createDataFrame([(11, \"Deepak\", 100),\n",
    "                               (12, \"Leena\", None),\n",
    "                               (13, \"Deepa\", 102),\n",
    "                               (15, \"Preety\", None)], [\"emp id\", \"emp name\", \"emp salary\"])\n",
    "\n",
    "nullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f949acf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emp id', 'emp name', 'emp salary']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = nullDF.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e687ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emp Id', 'Emp Name', 'Emp Salary']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.title() for i in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adeec208",
   "metadata": {},
   "outputs": [],
   "source": [
    "titleCaseCols = [''.join(j for j in i.title() if not j.isspace()) for i in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f81c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EmpId', 'EmpName', 'EmpSalary']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titleCaseCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab8ab762",
   "metadata": {},
   "outputs": [],
   "source": [
    "camleCaseCols = [column[0].lower()+column[1:] for column in titleCaseCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91154507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empId', 'empName', 'empSalary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camleCaseCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ef9875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+\n",
      "|empId|empName|empSalary|\n",
      "+-----+-------+---------+\n",
      "|   11| Deepak|      100|\n",
      "|   12|  Leena|     null|\n",
      "|   13|  Deepa|      102|\n",
      "|   15| Preety|     null|\n",
      "+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nullDF = nullDF.toDF(*camleCaseCols)\n",
    "\n",
    "nullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cec79f",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/company/justenough-spark/posts/?feedView=images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9b28f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+\n",
      "|empId|empName|empSalary|\n",
      "+-----+-------+---------+\n",
      "|   11| Deepak|      100|\n",
      "|   12|  Leena|     null|\n",
      "|   13|  Deepa|      102|\n",
      "|   15| Preety|     null|\n",
      "+-----+-------+---------+\n",
      "\n",
      "+-----+-------+---------+\n",
      "|empId|empName|empSalary|\n",
      "+-----+-------+---------+\n",
      "|   12|  Leena|     null|\n",
      "|   15| Preety|     null|\n",
      "+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "nullDF.show()\n",
    "\n",
    "nonNullDF = nullDF.filter(isnull(\"empSalary\"))\n",
    "\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19dd74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+\n",
      "|empId|empName|empSalary|\n",
      "+-----+-------+---------+\n",
      "|   12|  Leena|     null|\n",
      "|   15| Preety|     null|\n",
      "+-----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nonNullDF = nullDF.filter(col(\"empSalary\").isNull())\n",
    "\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce3ac19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dba4ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| _1|      _2|\n",
      "+---+--------+\n",
      "|  1|Subhayan|\n",
      "|  2|  Ankana|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 'Subhayan'), (2, 'Ankana')])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413bc51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'134217728b'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16bedf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.get(\"spark.driver.maxResultSize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c2bbed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>_1</th><th>_2</th></tr>\n",
       "<tr><td>1</td><td>Subhayan</td></tr>\n",
       "<tr><td>2</td><td>Ankana</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------+\n",
       "| _1|      _2|\n",
       "+---+--------+\n",
       "|  1|Subhayan|\n",
       "|  2|  Ankana|\n",
       "+---+--------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = display(df)\n",
    "\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950288bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|flatten(array(array(1, 2), array(3, 4)))|\n",
      "+----------------------------------------+\n",
      "|                            [1, 2, 3, 4]|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT flatten(array(array(1, 2), array(3, 4)))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83dcdf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|array(array(1, 2), array(3, 4))|\n",
      "+-------------------------------+\n",
      "|               [[1, 2], [3, 4]]|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT array(array(1, 2), array(3, 4))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a480334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|floored|\n",
      "+-------+\n",
      "|      5|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT floor(5.9) AS floored\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce37c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ceiled|\n",
      "+------+\n",
      "|     6|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ceil(5.9) AS ceiled\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467aa8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------+-----------------------------+--------------------------------+---------------------------------+\n",
      "|trim(    SparkSQL   )|TRIM(BOTH SL FROM SSparkSQLS)|TRIM(BOTH SL FROM SSparkSQLS)|TRIM(LEADING SL FROM SSparkSQLS)|TRIM(TRAILING SL FROM SSparkSQLS)|\n",
      "+---------------------+-----------------------------+-----------------------------+--------------------------------+---------------------------------+\n",
      "|             SparkSQL|                       parkSQ|                       parkSQ|                        parkSQLS|                         SSparkSQ|\n",
      "+---------------------+-----------------------------+-----------------------------+--------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT trim('    SparkSQL   '), \\\n",
    "       trim('SL', 'SSparkSQLS'), \\\n",
    "       trim(BOTH 'SL' FROM 'SSparkSQLS'), \\\n",
    "       trim(LEADING 'SL' FROM 'SSparkSQLS'), \\\n",
    "       trim(TRAILING 'SL' FROM 'SSparkSQLS')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd3be094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|letter|position|\n",
      "+------+--------+\n",
      "|     a|      10|\n",
      "|     a|      10|\n",
      "|     a|      20|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"letter\", StringType(), True),\n",
    "  StructField(\"position\", IntegerType(), True)])\n",
    "\n",
    "df = spark.createDataFrame([(\"a\", 10), (\"a\", 10), (\"a\", 20)], schema)\n",
    "windowSpec = Window.partitionBy(\"letter\").orderBy(\"position\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c263448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----------+----------+\n",
      "|letter|position|rank|dense_rank|row_number|\n",
      "+------+--------+----+----------+----------+\n",
      "|     a|      10|   1|         1|         1|\n",
      "|     a|      10|   1|         1|         2|\n",
      "|     a|      20|   3|         2|         3|\n",
      "+------+--------+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "  .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    "  .withColumn(\"row_number\", row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fa1728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|salary|\n",
      "+-----+---+------+\n",
      "| Ravi| 28|  3200|\n",
      "|Abdul| 23|  4800|\n",
      "| John| 32|  6500|\n",
      "| Rosy| 48|  8200|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"Ravi\", \"28\", \"3200\"),\n",
    " (\"Abdul\", \"23\", \"4800\"),\n",
    " (\"John\", \"32\", \"6500\"),\n",
    " (\"Rosy\", \"48\", \"8200\")]\n",
    " \n",
    "df = spark.createDataFrame(data_list).toDF(\"name\", \"age\", \"salary\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "159ea248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "| name|(salary * 20)|\n",
      "+-----+-------------+\n",
      "| Ravi|      64000.0|\n",
      "|Abdul|      96000.0|\n",
      "| John|     130000.0|\n",
      "| Rosy|     164000.0|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", col(\"salary\") * 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac3bd8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "| name|(salary * 20)|\n",
      "+-----+-------------+\n",
      "| Ravi|      64000.0|\n",
      "|Abdul|      96000.0|\n",
      "| John|     130000.0|\n",
      "| Rosy|     164000.0|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", expr(\"salary\") * 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebe6f095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('X',)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['dummy'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "981608d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2021-10-22|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(current_date().alias(\"today\"))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79947476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- today: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86ad7017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     today|week_later|\n",
      "+----------+----------+\n",
      "|2021-10-22|2021-10-29|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"week_later\", date_add(\"today\", 7))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "065dfa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     today|week_later|\n",
      "+----------+----------+\n",
      "|2021-10-22|2021-10-29|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 40604)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:34583)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/envs/beakerx/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"week_later\", date_add(to_date(\"today\", \"dd-MM-yyyy\"), 7))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e14bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
