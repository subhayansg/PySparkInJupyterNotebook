{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3484935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279c069",
   "metadata": {},
   "source": [
    "### Define the window specification\n",
    "\n",
    "`from pyspark.sql.window import Window`\n",
    "\n",
    "`# Defines partitioning specification and ordering specification.`\n",
    "`windowSpec = \\\n",
    "  Window \\\n",
    "    .partitionBy(...) \\\n",
    "    .orderBy(...)`\n",
    "    \n",
    "`# Defines a Window Specification with a ROW frame.`\n",
    "\n",
    "`windowSpec.rowsBetween(start, end)`\n",
    "\n",
    "`# Defines a Window Specification with a RANGE frame.`\n",
    "\n",
    "`windowSpec.rangeBetween(start, end)`\n",
    "\n",
    "Here, frame_type can be either ROWS (for ROW frame) or RANGE (for RANGE frame); \n",
    "\n",
    "start can be any of UNBOUNDED PRECEDING, CURRENT ROW, <value> PRECEDING, and <value> FOLLOWING; and end can be any of UNBOUNDED FOLLOWING, CURRENT ROW, <value> PRECEDING, and <value> FOLLOWING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9126329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeList = [(\"sales\", 1, 5000),\n",
    "(\"personnel\", 2, 3900),\n",
    "(\"sales\", 3, 4800),\n",
    "(\"sales\", 4, 4800),\n",
    "(\"personnel\", 5, 3500),\n",
    "(\"develop\", 7, 4200),\n",
    "(\"develop\", 8, 6000),\n",
    "(\"develop\", 9, 4500),\n",
    "(\"develop\", 10, 5200),\n",
    "(\"develop\", 11, 5200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcccf609",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDf = spark.createDataFrame(employeeList, [\"deptName\", \"empID\", \"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416a7a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "| deptName|empID|salary|\n",
      "+---------+-----+------+\n",
      "|    sales|    1|  5000|\n",
      "|personnel|    2|  3900|\n",
      "|    sales|    3|  4800|\n",
      "|    sales|    4|  4800|\n",
      "|personnel|    5|  3500|\n",
      "|  develop|    7|  4200|\n",
      "|  develop|    8|  6000|\n",
      "|  develop|    9|  4500|\n",
      "|  develop|   10|  5200|\n",
      "|  develop|   11|  5200|\n",
      "+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a7600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311a72d",
   "metadata": {},
   "source": [
    "### Get dept wise avg salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea9e9ae",
   "metadata": {},
   "source": [
    "___Using normal groupBy___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067aa382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "| deptName|      avg(salary)|\n",
      "+---------+-----------------+\n",
      "|  develop|           5020.0|\n",
      "|    sales|4866.666666666667|\n",
      "|personnel|           3700.0|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgSalDf = empDf. \\\n",
    "    groupBy(\"deptName\").\\\n",
    "    avg(\"salary\")\n",
    "\n",
    "avgSalDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f7d33",
   "metadata": {},
   "source": [
    "___Using window aggregation___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3e38cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "| deptName|          avg sal|\n",
      "+---------+-----------------+\n",
      "|  develop|           5020.0|\n",
      "|    sales|4866.666666666667|\n",
      "|personnel|           3700.0|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "avgSalDf2 = empDf. \\\n",
    "    withColumn(\"avg sal\", avg(\"salary\").over(Window.partitionBy(\"deptName\"))). \\\n",
    "    select(\"deptName\", \"avg sal\"). \\\n",
    "    distinct()\n",
    "\n",
    "avgSalDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d02bb0",
   "metadata": {},
   "source": [
    "### Top N per Group\n",
    "___Top N per Group is useful when you need to compute the first and second best-sellers in category.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a655b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prdList = [(\"Thin\", \"cell phone\", 6000),\n",
    "(\"Normal\", \"tablet\", 1500),\n",
    "(\"Mini\", \"tablet\", 5500),\n",
    "(\"Ultra thin\", \"cell phone\", 5000),\n",
    "(\"Very thin\", \"cell phone\", 6000),\n",
    "(\"Big\", \"tablet\", 2500),\n",
    "(\"Bendable\", \"cell phone\", 3000),\n",
    "(\"Foldable\", \"cell phone\", 3000),\n",
    "(\"Pro\", \"tablet\", 4500),\n",
    "(\"Pro2\", \"tablet\", 6500)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527503a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Thin|cell phone|   6000|\n",
      "|    Normal|    tablet|   1500|\n",
      "|      Mini|    tablet|   5500|\n",
      "|Ultra thin|cell phone|   5000|\n",
      "| Very thin|cell phone|   6000|\n",
      "|       Big|    tablet|   2500|\n",
      "|  Bendable|cell phone|   3000|\n",
      "|  Foldable|cell phone|   3000|\n",
      "|       Pro|    tablet|   4500|\n",
      "|      Pro2|    tablet|   6500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prdDf = spark.createDataFrame(prdList, [\"product\", \"category\", \"revenue\"])\n",
    "\n",
    "prdDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07473b",
   "metadata": {},
   "source": [
    "___What are the best-selling and the second best-selling products in every category?___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46c4789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_selling_window = Window.partitionBy(\"category\"). \\\n",
    "    orderBy(col(\"revenue\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e93f230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Pro2|    tablet|   6500|\n",
      "|      Mini|    tablet|   5500|\n",
      "|      Thin|cell phone|   6000|\n",
      "| Very thin|cell phone|   6000|\n",
      "|Ultra thin|cell phone|   5000|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prdDf.withColumn(\"rank\", dense_rank().over(best_selling_window)). \\\n",
    "    filter((col(\"rank\") == 1) | (col(\"rank\") == 2)). \\\n",
    "    drop(\"rank\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4939fefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+\n",
      "|  product|  category|revenue|\n",
      "+---------+----------+-------+\n",
      "|     Pro2|    tablet|   6500|\n",
      "|     Thin|cell phone|   6000|\n",
      "|Very thin|cell phone|   6000|\n",
      "+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prdDf.select(\"product\", \"category\", \"revenue\", (dense_rank().over(best_selling_window)).alias(\"best selling product\")). \\\n",
    "    filter(col(\"best selling product\") == 1). \\\n",
    "    drop(\"best selling product\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48671d8",
   "metadata": {},
   "source": [
    "___What is the difference between the revenue of each product and the revenue of the best-selling product in the same category of that product?___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a52856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+---------------------+\n",
      "|   product|  category|revenue|difference in revenue|\n",
      "+----------+----------+-------+---------------------+\n",
      "|      Pro2|    tablet|   6500|                    0|\n",
      "|      Mini|    tablet|   5500|                 1000|\n",
      "|       Pro|    tablet|   4500|                 2000|\n",
      "|       Big|    tablet|   2500|                 4000|\n",
      "|    Normal|    tablet|   1500|                 5000|\n",
      "|      Thin|cell phone|   6000|                    0|\n",
      "| Very thin|cell phone|   6000|                    0|\n",
      "|Ultra thin|cell phone|   5000|                 1000|\n",
      "|  Bendable|cell phone|   3000|                 3000|\n",
      "|  Foldable|cell phone|   3000|                 3000|\n",
      "+----------+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue_difference = max(prdDf[\"revenue\"]).over(best_selling_window) - prdDf[\"revenue\"]\n",
    "prdDf.select(\"product\", \"category\", \"revenue\", revenue_difference.alias(\"difference in revenue\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ae618",
   "metadata": {},
   "source": [
    "### Creating boundary for the window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40988e5",
   "metadata": {},
   "source": [
    "There are five types of boundaries, which are:\n",
    "`UNBOUNDED PRECEDING, UNBOUNDED FOLLOWING, CURRENT ROW, <value> PRECEDING, and <value> FOLLOWING`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0307b545",
   "metadata": {},
   "source": [
    "There are two types of frames, `ROW` frame and `RANGE` frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bf07c",
   "metadata": {},
   "source": [
    "If `CURRENT ROW` is used as a boundary, it represents the current input row. `<value> PRECEDING` and `<value> FOLLOWING` describes the number of rows appear before and after the current input row, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b5ea9",
   "metadata": {},
   "source": [
    "### RANGE frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87829635",
   "metadata": {},
   "source": [
    "RANGE frames are based on logical offsets from the position of the current input row\n",
    "\n",
    "A logical offset is the difference between the value of the ordering expression of the current input row and the value of that same expression of the boundary row of the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13e189",
   "metadata": {},
   "source": [
    "Because of this definition, when a RANGE frame is used, only a single ordering expression is allowed. Also, for a RANGE frame, all rows having the same value of the ordering expression with the current input row are considered as same row as far as the boundary calculation is concerned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d4d8c",
   "metadata": {},
   "source": [
    "<img src=\"RangePartition/r1.png\">\n",
    "<img src=\"RangePartition/r2.png\">\n",
    "<img src=\"RangePartition/r3.png\">\n",
    "<img src=\"RangePartition/r4.png\">\n",
    "<img src=\"RangePartition/r5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6032bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Thin|cell phone|   6000|\n",
      "|    Normal|    tablet|   1500|\n",
      "|      Mini|    tablet|   5500|\n",
      "|Ultra thin|cell phone|   5000|\n",
      "| Very thin|cell phone|   6000|\n",
      "|       Big|    tablet|   2500|\n",
      "|  Bendable|cell phone|   3000|\n",
      "|  Foldable|cell phone|   3000|\n",
      "|       Pro|    tablet|   4500|\n",
      "|      Pro2|    tablet|   6500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prdDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f0490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rangeWindow = Window.partitionBy(\"category\").orderBy(\"revenue\").rangeBetween(1000,  3000)\n",
    "rowWindow = Window.partitionBy(\"category\").orderBy(\"revenue\").rowsBetween(1,  2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69f559fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------------+-----------+\n",
      "|   product|  category|revenue|range revenue|row revenue|\n",
      "+----------+----------+-------+-------------+-----------+\n",
      "|    Normal|    tablet|   1500|         7000|       7000|\n",
      "|       Big|    tablet|   2500|        10000|      10000|\n",
      "|       Pro|    tablet|   4500|        12000|      12000|\n",
      "|      Mini|    tablet|   5500|         6500|       6500|\n",
      "|      Pro2|    tablet|   6500|         null|       null|\n",
      "|  Bendable|cell phone|   3000|        17000|       8000|\n",
      "|  Foldable|cell phone|   3000|        17000|      11000|\n",
      "|Ultra thin|cell phone|   5000|        12000|      12000|\n",
      "|      Thin|cell phone|   6000|         null|       6000|\n",
      "| Very thin|cell phone|   6000|         null|       null|\n",
      "+----------+----------+-------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prdDf.withColumn(\"range revenue\", sum(\"revenue\").over(rangeWindow)). \\\n",
    "    withColumn(\"row revenue\", sum(\"revenue\").over(rowWindow)). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd291830",
   "metadata": {},
   "source": [
    "### top 3 products having highest sales on monthly basis\n",
    "Consider there is a dataset of 1 year shopping details of a clothes store with multiple categories of clothes.\n",
    "\n",
    "Header: Timestamp, transactionid,categoryname,totalsales\n",
    "\n",
    "Timestamp is daily,so each category has 365rows.\n",
    "\n",
    "Problem is write a spark code to get the top 3 products having highest sales on monthly basis\n",
    "\n",
    "```\n",
    "denseSpec = Window.partitionBy(\"month\").orderBy(col(\"sum_sales\").desc )\n",
    "\n",
    "df2 = df.groupBy(\"month\", \"category\") \n",
    "      .sum(\"sales\" )\n",
    "      .withColumnRenamed(\"sum(sales)\", \"sum_sales\")\n",
    "      .withColumn(\"ranks\", dense_rank().over(denseSpec))\n",
    "      .where(\"ranks <=3\")\n",
    "      .orderBy(\"month\", \"ranks\")\n",
    "      .show(false)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e95577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
