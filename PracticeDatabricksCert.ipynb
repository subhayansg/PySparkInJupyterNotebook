{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4891a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad86f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b355d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c659bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.mode(\"overwrite\").save(\"/user/itv736079/test/data/flights_delay.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce1100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 itv736079 supergroup          0 2021-10-20 09:44 /user/itv736079/test/data/flights_delay.csv/_SUCCESS\n",
      "-rw-r--r--   3 itv736079 supergroup     487972 2021-10-20 09:44 /user/itv736079/test/data/flights_delay.csv/part-00000-982e1285-26f8-4ba5-8623-33045bc9847f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/itv736079/test/data/flights_delay.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcde030",
   "metadata": {},
   "source": [
    "#### You are given a DataFrame which looks like below.\n",
    "\n",
    "`\n",
    "+---+-----+------+----+----------+\n",
    "| ID|FName| LName| DOB|Department|\n",
    "+---+-----+------+----+----------+\n",
    "|101| John| Doe|1977| Software|\n",
    "|102|David|Turner|1984| Support|\n",
    "|103|Abdul| Hamid|1978| Account|\n",
    "+---+-----+------+----+----------+\n",
    "`\n",
    "\n",
    "#### You are given a task to transform this DataFrame to the following structure.\n",
    "\n",
    "`\n",
    "+---+---------------------+----------+\n",
    "|ID |PersonalDetails |Department|\n",
    "+---+---------------------+----------+\n",
    "|101|[John, Doe, 1977] |Software |\n",
    "|102|[David, Turner, 1984]|Support |\n",
    "|103|[Abdul, Hamid, 1978] |Account |\n",
    "+---+---------------------+----------+\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cbb9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(101,'John','Doe',1977,'Software'),\n",
    "(102,'David','Turner',1984,'Support'),\n",
    "(103,'Abdul','Hamid',1978,'Account')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e7f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data, ['ID', 'FName', 'LName', 'DOB', 'Department'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f061e829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----+----------+\n",
      "| ID|FName| LName| DOB|Department|\n",
      "+---+-----+------+----+----------+\n",
      "|101| John|   Doe|1977|  Software|\n",
      "|102|David|Turner|1984|   Support|\n",
      "|103|Abdul| Hamid|1978|   Account|\n",
      "+---+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27341a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "value should be a float, int, long, string, bool or dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ba0c29d6ceef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LName'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DOB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Department'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfill\u001b[0;34m(self, value, subset)\u001b[0m\n\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m     \u001b[0mfill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, subset)\u001b[0m\n\u001b[1;32m   1753\u001b[0m         \"\"\"\n\u001b[1;32m   1754\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1755\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value should be a float, int, long, string, bool or dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m         \u001b[0;31m# Note that bool validates isinstance(int), but we don't want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: value should be a float, int, long, string, bool or dict"
     ]
    }
   ],
   "source": [
    "data = [(101,'John','Doe',None,'Software'),\n",
    "(102,'David','Turner',None,'Support'),\n",
    "(103,'Abdul','Hamid',1978,'Account')]\n",
    "\n",
    "df1 = spark.createDataFrame(data, ['ID', 'FName', 'LName', 'DOB', 'Department'])\n",
    "\n",
    "df1.na.fill(None)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef169b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93a2222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+----------+\n",
      "|id |PersonalDetails      |Department|\n",
      "+---+---------------------+----------+\n",
      "|101|[John, Doe, 1977]    |Software  |\n",
      "|102|[David, Turner, 1984]|Support   |\n",
      "|103|[Abdul, Hamid, 1978] |Account   |\n",
      "+---+---------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.select(\"id\", F.struct(\"FName\", \"LName\", \"DOB\").alias(\"PersonalDetails\"), \"Department\")\n",
    "\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647e3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+----------+\n",
      "|id |PersonalDetails      |Department|\n",
      "+---+---------------------+----------+\n",
      "|101|[John, Doe, 1977]    |Software  |\n",
      "|102|[David, Turner, 1984]|Support   |\n",
      "|103|[Abdul, Hamid, 1978] |Account   |\n",
      "+---+---------------------+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- PersonalDetails: struct (nullable = false)\n",
      " |    |-- FName: string (nullable = true)\n",
      " |    |-- LName: string (nullable = true)\n",
      " |    |-- DOB: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.selectExpr(\"id\", \"struct(FName, LName, DOB) as PersonalDetails\", \"Department\")\n",
    "\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef84eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|PersonalDetails.FName|\n",
      "+---------------------+\n",
      "|                 John|\n",
      "|                David|\n",
      "|                Abdul|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(F.col(\"PersonalDetails\").getField(\"FName\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "177a6f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+----------+\n",
      "|id |PersonalDetails      |Department|\n",
      "+---+---------------------+----------+\n",
      "|101|[John, Doe, 1977]    |Software  |\n",
      "|102|[David, Turner, 1984]|Support   |\n",
      "|103|[Abdul, Hamid, 1978] |Account   |\n",
      "+---+---------------------+----------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- PersonalDetails: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.selectExpr(\"id\", \"Array(FName, LName, DOB) as PersonalDetails\", \"Department\")\n",
    "\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b07026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('X',)]\n",
    "\n",
    "df = spark.createDataFrame(data, ['dummy'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ca0fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2021-10-20|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(F.current_date().alias(\"today\"))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c791b7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     today|  week_ago|\n",
      "+----------+----------+\n",
      "|2021-10-20|2021-10-13|\n",
      "+----------+----------+\n",
      "\n",
      "root\n",
      " |-- today: date (nullable = false)\n",
      " |-- week_ago: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"week_ago\", F.date_sub(F.col(\"today\"), 7))\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "271fbe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     today|  week_ago|\n",
      "+----------+----------+\n",
      "|2021-10-20|2021-10-13|\n",
      "+----------+----------+\n",
      "\n",
      "root\n",
      " |-- today: date (nullable = false)\n",
      " |-- week_ago: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"week_ago\", F.col(\"today\")- 7)\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17c95c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     today|  week_ago|\n",
      "+----------+----------+\n",
      "|2021-10-20|2021-10-13|\n",
      "+----------+----------+\n",
      "\n",
      "root\n",
      " |-- today: date (nullable = false)\n",
      " |-- week_ago: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"week_ago\", F.expr(\"today - 7\"))\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf8adb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(order_id)</th></tr>\n",
       "<tr><td>68883</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------+\n",
       "|count(order_id)|\n",
       "+---------------+\n",
       "|          68883|\n",
       "+---------------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.select(\"order_id\").distinct().agg(F.count(\"order_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f5307ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders.selectExpr(\"countDistinct(order_id)\")\n",
    "\n",
    "# this will fail as countDistinct() function is not available in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7e9f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(DISTINCT order_id)</th></tr>\n",
       "<tr><td>68883</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------------+\n",
       "|count(DISTINCT order_id)|\n",
       "+------------------------+\n",
       "|                   68883|\n",
       "+------------------------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.select(F.countDistinct(\"order_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a08e1678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(DISTINCT order_id)</th></tr>\n",
       "<tr><td>68883</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------------+\n",
       "|count(DISTINCT order_id)|\n",
       "+------------------------+\n",
       "|                   68883|\n",
       "+------------------------+"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.selectExpr(\"count(distinct(order_id))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08d027fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------+\n",
      "|order_id|order_customer_id|increment|\n",
      "+--------+-----------------+---------+\n",
      "|       1|            11599|        0|\n",
      "|       2|              256|      100|\n",
      "|       3|            12111|        0|\n",
      "|       4|             8827|        0|\n",
      "|       5|            11318|        0|\n",
      "+--------+-----------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.selectExpr(\"order_id\", \"order_customer_id\", \"if(order_customer_id < 5000, order_id * 50 , 0) as increment\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34c88f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|extend|\n",
      "+--------+--------------------+-----------------+---------------+------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     5|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|    10|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|    15|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|    20|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|    25|\n",
      "+--------+--------------------+-----------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.select(\"*\", F.expr(\"order_id * 5 as extend\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d1ceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|extend|\n",
      "+--------+--------------------+-----------------+---------------+------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|     5|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|    10|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|    15|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|    20|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|    25|\n",
      "+--------+--------------------+-----------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.selectExpr(\"*\", \"order_id * 5 as extend\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3487116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+\n",
      "|       Country|Week|Quantity|\n",
      "+--------------+----+--------+\n",
      "|       Germany|  48|      10|\n",
      "|       Germany|  49|       5|\n",
      "|       Germany|  50|       3|\n",
      "|       Germany|  51|       2|\n",
      "|United Kingdom|  48|       2|\n",
      "|United Kingdom|  49|       2|\n",
      "+--------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"Germany\", 48, 10),\n",
    " (\"Germany\", 49, 5),\n",
    " (\"Germany\", 50, 3),\n",
    " (\"Germany\", 51, 2),\n",
    " (\"United Kingdom\", 48, 2),\n",
    " (\"United Kingdom\", 49, 2)]\n",
    " \n",
    "df4 = spark.createDataFrame(data_list).toDF(\"Country\", \"Week\", \"Quantity\")\n",
    "\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb023540",
   "metadata": {},
   "source": [
    "#### Calculate 3 weeks running total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30191437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+----------+\n",
      "|       Country|Week|Quantity|3WeekTotal|\n",
      "+--------------+----+--------+----------+\n",
      "|       Germany|  48|      10|      null|\n",
      "|       Germany|  49|       5|      null|\n",
      "|       Germany|  50|       3|      null|\n",
      "|       Germany|  51|       2|      null|\n",
      "|United Kingdom|  48|       2|      null|\n",
      "|United Kingdom|  49|       2|      null|\n",
      "+--------------+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "running_total_window = Window. \\\n",
    "    partitionBy(\"Country\"). \\\n",
    "    orderBy(\"Week\"). \\\n",
    "    rowsBetween(2, Window.currentRow)\n",
    " \n",
    "df4.withColumn(\"3WeekTotal\", F.sum(\"Quantity\").over(running_total_window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "793a5d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+----------+\n",
      "|       Country|Week|Quantity|3WeekTotal|\n",
      "+--------------+----+--------+----------+\n",
      "|       Germany|  48|      10|        10|\n",
      "|       Germany|  49|       5|        15|\n",
      "|       Germany|  50|       3|        18|\n",
      "|       Germany|  51|       2|        10|\n",
      "|United Kingdom|  48|       2|         2|\n",
      "|United Kingdom|  49|       2|         4|\n",
      "+--------------+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "running_total_window = Window. \\\n",
    "    partitionBy(\"Country\"). \\\n",
    "    orderBy(\"Week\"). \\\n",
    "    rowsBetween(-2, Window.currentRow)\n",
    " \n",
    "df4.withColumn(\"3WeekTotal\", F.sum(\"Quantity\").over(running_total_window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24ef5ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Week: long (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66aa8ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+\n",
      "|       Country|Week|Quantity|\n",
      "+--------------+----+--------+\n",
      "|       Germany|  48|      10|\n",
      "|       Germany|  49|       5|\n",
      "|       Germany|  50|       3|\n",
      "|       Germany|  51|       2|\n",
      "|United Kingdom|  48|       2|\n",
      "|United Kingdom|  49|       2|\n",
      "+--------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumnRenamed('Week3', 'UpdatedWeek').show()\n",
    "\n",
    "# no error will be thrown even if the column to be renamed does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4341eb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+\n",
      "|       Country|Week|Quantity|\n",
      "+--------------+----+--------+\n",
      "|       Germany|  48|      10|\n",
      "|       Germany|  49|       5|\n",
      "|       Germany|  50|       3|\n",
      "|       Germany|  51|       2|\n",
      "|United Kingdom|  48|       2|\n",
      "|United Kingdom|  49|       2|\n",
      "+--------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.drop('Week3').show()\n",
    "\n",
    "# no error will be thrown even if the column to be dropped does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "425bbdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Country</th><th>Week</th><th>Quantity</th></tr>\n",
       "<tr><td>Germany</td><td>51</td><td>2</td></tr>\n",
       "<tr><td>Germany</td><td>49</td><td>5</td></tr>\n",
       "<tr><td>Germany</td><td>48</td><td>10</td></tr>\n",
       "<tr><td>Germany</td><td>50</td><td>3</td></tr>\n",
       "<tr><td>United Kingdom</td><td>48</td><td>2</td></tr>\n",
       "<tr><td>United Kingdom</td><td>49</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+----+--------+\n",
       "|       Country|Week|Quantity|\n",
       "+--------------+----+--------+\n",
       "|       Germany|  51|       2|\n",
       "|       Germany|  49|       5|\n",
       "|       Germany|  48|      10|\n",
       "|       Germany|  50|       3|\n",
       "|United Kingdom|  48|       2|\n",
       "|United Kingdom|  49|       2|\n",
       "+--------------+----+--------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c209ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+\n",
      "|       Country|Week|Quantity|\n",
      "+--------------+----+--------+\n",
      "|       Germany|  48|      10|\n",
      "|       Germany|  49|       5|\n",
      "|       Germany|  50|       3|\n",
      "|       Germany|  51|       2|\n",
      "|United Kingdom|  48|       2|\n",
      "|United Kingdom|  49|       2|\n",
      "+--------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.registerTempTable(\"dfTable\")\n",
    "\n",
    "spark.sql(\"select * from dfTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fddf63d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+\n",
      "|ID |TEXT                              |\n",
      "+---+----------------------------------+\n",
      "|101|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|102|WHITE LANTERN                     |\n",
      "|103|RED WOOLLY HOTTIE WHITE HEART     |\n",
      "+---+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(101,'WHITE HANGING HEART T-LIGHT HOLDER'),\n",
    "(102,'WHITE LANTERN'),\n",
    "(103,'RED WOOLLY HOTTIE WHITE HEART')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['ID','TEXT'])\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c16699f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------+\n",
      "|ID |VALUES                                  |\n",
      "+---+----------------------------------------+\n",
      "|101|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|102|[WHITE, LANTERN]                        |\n",
      "|103|[RED, WOOLLY, HOTTIE, WHITE, HEART]     |\n",
      "+---+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(\"ID\", split(col(\"TEXT\"), \" \").alias(\"VALUES\"))\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50cc70a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+------+\n",
      "|ID |V1   |V2     |V3    |\n",
      "+---+-----+-------+------+\n",
      "|101|WHITE|HANGING|HEART |\n",
      "|102|WHITE|LANTERN|null  |\n",
      "|103|RED  |WOOLLY |HOTTIE|\n",
      "+---+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.selectExpr(\"ID\", \"VALUES[0] as V1\", \"VALUES[1] as V2\", \"VALUES[2] as V3\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7554f60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7f4190c226d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "513e1f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=101, TEXT='WHITE HANGING HEART T-LIGHT HOLDER'),\n",
       " Row(ID=102, TEXT='WHITE LANTERN'),\n",
       " Row(ID=103, TEXT='RED WOOLLY HOTTIE WHITE HEART')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d23a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 3, 4,  25, 1, lit(None)),\n",
    "(2, 6, 7,   2, 2, lit(None)),\n",
    "(3, 3, lit(None),  25, 3, lit(None)),\n",
    "(4,  lit(None), lit(None),   3, 2, lit(None)),\n",
    "(5,  lit(None), lit(None),   lit(None), 2, lit(None)),\n",
    "(6, 3, 2,  25, 2, lit(None))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c620dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactionsDf = spark.createDataFrame(data, ['transactionId', 'predError', 'value', 'storeId', 'productId', 'f'])\n",
    "\n",
    "# # transactionsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5188b5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------+---------+---+\n",
      "|transactionId|predError|value|storeId|productId|  f|\n",
      "+-------------+---------+-----+-------+---------+---+\n",
      "|            1|        3|    4|     25|        1| 55|\n",
      "|            2|        6|    7|      2|        2| 55|\n",
      "|            3|        3|   55|     25|        3| 55|\n",
      "|            4|       55|   55|      3|        2| 55|\n",
      "|            5|       55|   55|     55|        2| 55|\n",
      "|            6|        3|    2|     25|        2| 55|\n",
      "+-------------+---------+-----+-------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 3, 4,  25, 1, 55),\n",
    "(2, 6, 7,   2, 2, 55),\n",
    "(3, 3, 55,  25, 3, 55),\n",
    "(4,  55, 55,   3, 2, 55),\n",
    "(5,  55, 55,   55, 2, 55),\n",
    "(6, 3, 2,  25, 2, 55)]\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data, ['transactionId', 'predError', 'value', 'storeId', 'productId', 'f'])\n",
    "\n",
    "transactionsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a11cd37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+---+\n",
      "|transactionId|predError|value|  f|\n",
      "+-------------+---------+-----+---+\n",
      "|            1|        3|    4| 55|\n",
      "|            2|        6|    7| 55|\n",
      "|            3|        3|   55| 55|\n",
      "|            4|       55|   55| 55|\n",
      "|            5|       55|   55| 55|\n",
      "|            6|        3|    2| 55|\n",
      "+-------------+---------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionsDf.select(\"transactionId\", \"predError\", \"value\", \"f\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "617347d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+---+\n",
      "|transactionId|predError|value|  f|\n",
      "+-------------+---------+-----+---+\n",
      "|            1|        3|    4| 55|\n",
      "|            2|        6|    7| 55|\n",
      "|            3|        3|   55| 55|\n",
      "|            4|       55|   55| 55|\n",
      "|            5|       55|   55| 55|\n",
      "|            6|        3|    2| 55|\n",
      "+-------------+---------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionsDf.select([\"transactionId\", \"predError\", \"value\", \"f\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c915925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
