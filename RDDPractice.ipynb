{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd368801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bd400c",
   "metadata": {},
   "source": [
    "### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce84c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years_logs = [\n",
    "  '2015-09-01 10:00:01|Error|Ac #3211001 ATW 10000 INR', \n",
    "  '2015-09-02 10:00:07|Info|Ac #3281001 ATW 11000 INR',\n",
    "  '2015-10-01 10:00:09|error|Ac #3311001 AWT 10500 INR', \n",
    "  '2015-11-01 10:00:01|error|Ac #3211001 AWT 10000 INR',\n",
    "  '2016-09-01 10:00:01|info|Ac #3211001 AWT 5000 INR', \n",
    "  '2016-09-02 10:00:01|ERROR|Ac #3211001 AWT 10000 INR',\n",
    "  '2016-10-01 10:00:01|error|Ac #3211001 AWT 8000 INR', \n",
    "  '2016-11-01 10:00:01|error|Ac #3211001 AWT 10000 INR',\n",
    "  '2016-12-01 10:00:01|Error|Ac #8211001 AWT 80000 INR', \n",
    "  '2016-12-02 10:00:01|error|Ac #9211001 AWT 90000 INR',\n",
    "  '2016-12-10 10:00:01|error|Ac #3811001 AWT 15000 INR', \n",
    "  '2016-12-01 10:00:01|info|Ac #3219001 AWT 16000 INR'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2426b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years_logs_rdd = spark.sparkContext.parallelize(last_years_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3be55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2015-09-01 10:00:01|Error|Ac #3211001 ATW 10000 INR',\n",
       " '2015-09-02 10:00:07|Info|Ac #3281001 ATW 11000 INR',\n",
       " '2015-10-01 10:00:09|error|Ac #3311001 AWT 10500 INR',\n",
       " '2015-11-01 10:00:01|error|Ac #3211001 AWT 10000 INR',\n",
       " '2016-09-01 10:00:01|info|Ac #3211001 AWT 5000 INR',\n",
       " '2016-09-02 10:00:01|ERROR|Ac #3211001 AWT 10000 INR',\n",
       " '2016-10-01 10:00:01|error|Ac #3211001 AWT 8000 INR',\n",
       " '2016-11-01 10:00:01|error|Ac #3211001 AWT 10000 INR',\n",
       " '2016-12-01 10:00:01|Error|Ac #8211001 AWT 80000 INR',\n",
       " '2016-12-02 10:00:01|error|Ac #9211001 AWT 90000 INR',\n",
       " '2016-12-10 10:00:01|error|Ac #3811001 AWT 15000 INR',\n",
       " '2016-12-01 10:00:01|info|Ac #3219001 AWT 16000 INR']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_years_logs_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbe5d6",
   "metadata": {},
   "source": [
    "___Find error log___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944848e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "error_log_count = 0\n",
    "last_years_logs_rdd. \\\n",
    "    foreach(lambda x: (error_log_count+1) if 'error' in x.lower() else error_log_count)\n",
    "print(error_log_count)\n",
    "\n",
    "# Here output is 0 because, the error_log_lines counter is printing the local value from the driver as it is not a distributed counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "093bd496",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_log_count = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e003e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_years_logs_rdd. \\\n",
    "    foreach(lambda x: (error_log_count.add(1) if 'error' in x.lower() else error_log_count.add(0)))\n",
    "error_log_count.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1541f",
   "metadata": {},
   "source": [
    "___How many error in 2016?___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64415372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "error_cnt_2016 = last_years_logs_rdd. \\\n",
    "    filter(lambda x: 'error' in x.lower() and '2016' in x). \\\n",
    "    count()\n",
    "print(error_cnt_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79310bf7",
   "metadata": {},
   "source": [
    "### Word count problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f7a01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsRdd = spark.sparkContext.textFile('/user/itv736079/WordCount.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67436081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning Spark',\n",
       " 'Spark uses RDDs',\n",
       " 'RDDs are crude',\n",
       " 'Spark is great',\n",
       " 'spark is fun']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "179dc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "updRdd = wordsRdd. \\\n",
    "        flatMap(lambda x: x.split(\" \")). \\\n",
    "        map(lambda x: x.lower()).\\\n",
    "        map(lambda x: (x,1)). \\\n",
    "        reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fd2103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1),\n",
       " ('am', 1),\n",
       " ('learning', 1),\n",
       " ('uses', 1),\n",
       " ('are', 1),\n",
       " ('is', 2),\n",
       " ('spark', 4),\n",
       " ('great', 1),\n",
       " ('fun', 1),\n",
       " ('rdds', 2),\n",
       " ('crude', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d063da",
   "metadata": {},
   "source": [
    "### map vs flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979113a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'am', 'learning', 'Spark'],\n",
       " ['Spark', 'uses', 'RDDs'],\n",
       " ['RDDs', 'are', 'crude'],\n",
       " ['Spark', 'is', 'great'],\n",
       " ['spark', 'is', 'fun']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappedRdd = wordsRdd. \\\n",
    "        map(lambda x: x.split(\" \")).collect()\n",
    "\n",
    "mappedRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a86cb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'RDDs',\n",
       " 'RDDs',\n",
       " 'are',\n",
       " 'crude',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'great',\n",
       " 'spark',\n",
       " 'is',\n",
       " 'fun']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatMappedRdd = wordsRdd. \\\n",
    "        flatMap(lambda x: x.split(\" \")).collect()\n",
    "\n",
    "flatMappedRdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353b647",
   "metadata": {},
   "source": [
    "### Find the sentence which is having the maximum number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a2ae81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['I', 'am', 'learning', 'Spark'], 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topK=1\n",
    "wordsRdd. \\\n",
    "    map(lambda x: x.split(\" \")). \\\n",
    "    map(lambda x: (x, len(x))). \\\n",
    "    takeOrdered(topK, key = lambda sentence_wordslength: -sentence_wordslength[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c400c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newWordsRdd = spark.sparkContext.parallelize([\"This is me\", \"This is a test\", \"Holy cow\", \"This sentence has most words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c87f7569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['This', 'sentence', 'has', 'most', 'words'], 5)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newWordsRdd. \\\n",
    "    map(lambda x: x.split(\" \")). \\\n",
    "    map(lambda x: (x, len(x))). \\\n",
    "    takeOrdered(1, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a1484",
   "metadata": {},
   "source": [
    "### Find the sentence which is having the least number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d781502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Holy', 'cow'], 2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newWordsRdd. \\\n",
    "    map(lambda x: x.split(\" \")). \\\n",
    "    map(lambda x: (x, len(x))). \\\n",
    "    takeOrdered(1, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9a6ab36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultMinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3be5834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f2d33",
   "metadata": {},
   "source": [
    "### textFile vs wholeTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b1fb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning Spark',\n",
       " 'Spark uses RDDs',\n",
       " 'RDDs are crude',\n",
       " 'Spark is great',\n",
       " 'spark is fun']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.textFile('/user/itv736079/WordCount.txt').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75dbdaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hdfs://m01.itversity.com:9000/user/itv736079/WordCount.txt',\n",
       "  'I am learning Spark\\r\\nSpark uses RDDs\\r\\nRDDs are crude\\r\\nSpark is great\\r\\nspark is fun')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.wholeTextFiles('/user/itv736079/WordCount.txt').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd4de60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_rdd = spark.sparkContext.range(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b2e0cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blank_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2b1c878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = spark.sparkContext.parallelize(range(1, 11))  # RDD of int\n",
    "num_rdd.getNumPartitions() # int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6345c6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb3ba018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd_parts = num_rdd.glom()\n",
    "num_rdd_parts.collect()\n",
    "\n",
    "# glom returns an RDD created by coalescing all elements within each partition into a list.\n",
    "# since default partitions=2, it will create 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2920688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = spark.sparkContext.parallelize(range(1, 11), 8)  # 8 partitions\n",
    "num_rdd.getNumPartitions() # int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0e6a094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3abe5c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4, 5], [6], [7], [8], [9, 10]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd_parts = num_rdd.glom()\n",
    "num_rdd_parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6bf5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
