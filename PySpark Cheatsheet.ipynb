{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6f2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e28c0c",
   "metadata": {},
   "source": [
    "### Different methods of adding schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c56f7f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "___DDL method___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584f5359",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf = spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8533faa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb47553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b29b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightSchemaDDL = \"\"\"FL_DATE DATE, OP_CARRIER STRING, OP_CARRIER_FL_NUM INT, ORIGIN STRING, \n",
    "      ORIGIN_CITY_NAME STRING, DEST STRING, DEST_CITY_NAME STRING, CRS_DEP_TIME INT, DEP_TIME INT, \n",
    "      WHEELS_ON INT, TAXI_IN INT, CRS_ARR_TIME INT, ARR_TIME INT, CANCELLED INT, DISTANCE INT\"\"\"\n",
    "\n",
    "flightTimeJsonDF = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(flightSchemaDDL) \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\\n",
    "    .load(\"/user/itv736079/flightdata/flight*.json\")\n",
    "\n",
    "flightTimeJsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbca3ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ARR_TIME: long (nullable = true)\n",
      " |-- CANCELLED: long (nullable = true)\n",
      " |-- CRS_ARR_TIME: long (nullable = true)\n",
      " |-- CRS_DEP_TIME: long (nullable = true)\n",
      " |-- DEP_TIME: long (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- DISTANCE: long (nullable = true)\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: long (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- TAXI_IN: long (nullable = true)\n",
      " |-- WHEELS_ON: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeJsonDF2 = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\\n",
    "    .load(\"/user/itv736079/flightdata/flight*.json\")\n",
    "\n",
    "flightTimeJsonDF2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96d1e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|FL_DATE   |OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|2000-01-01|DL        |1451             |BOS   |Boston, MA      |ATL |Atlanta, GA   |1115        |1113    |1343     |5      |1400        |1348    |0        |946     |\n",
      "|2000-01-01|DL        |1479             |BOS   |Boston, MA      |ATL |Atlanta, GA   |1315        |1311    |1536     |7      |1559        |1543    |0        |946     |\n",
      "|2000-01-01|DL        |1857             |BOS   |Boston, MA      |ATL |Atlanta, GA   |1415        |1414    |1642     |9      |1721        |1651    |0        |946     |\n",
      "|2000-01-01|DL        |1997             |BOS   |Boston, MA      |ATL |Atlanta, GA   |1715        |1720    |1955     |10     |2013        |2005    |0        |946     |\n",
      "|2000-01-01|DL        |2065             |BOS   |Boston, MA      |ATL |Atlanta, GA   |2015        |2010    |2230     |10     |2300        |2240    |0        |946     |\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeJsonDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d7f249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|ARR_TIME|CANCELLED|CRS_ARR_TIME|CRS_DEP_TIME|DEP_TIME|DEST|DEST_CITY_NAME|DISTANCE| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|TAXI_IN|WHEELS_ON|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|    1348|        0|        1400|        1115|    1113| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1451|   BOS|      Boston, MA|      5|     1343|\n",
      "|    1543|        0|        1559|        1315|    1311| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1479|   BOS|      Boston, MA|      7|     1536|\n",
      "|    1651|        0|        1721|        1415|    1414| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1857|   BOS|      Boston, MA|      9|     1642|\n",
      "|    2005|        0|        2013|        1715|    1720| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1997|   BOS|      Boston, MA|     10|     1955|\n",
      "|    2240|        0|        2300|        2015|    2010| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             2065|   BOS|      Boston, MA|     10|     2230|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeJsonDF2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d1377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDF = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"/user/itv736079/flightdata/flight*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305d1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeParquetDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17495a7b",
   "metadata": {},
   "source": [
    "___For JSON, parquet, Avro data, schema definition can be implicit, as can be seen from above___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7e446",
   "metadata": {},
   "source": [
    "### Programmatically defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd452f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_CITY_NAME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_CITY_NAME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- DEP_TIME: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- ARR_TIME: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "flightSchemaStruct = StructType([\n",
    "    StructField(\"FL_DATE\", DateType(), False),\n",
    "    StructField(\"OP_CARRIER\", StringType()),\n",
    "    StructField(\"OP_CARRIER_FL_NUM\", IntegerType()),\n",
    "    StructField(\"ORIGIN\", StringType()),\n",
    "    StructField(\"ORIGIN_CITY_NAME\", StringType()),\n",
    "    StructField(\"DEST\", StringType()),\n",
    "    StructField(\"DEST_CITY_NAME\", StringType()),\n",
    "    StructField(\"CRS_DEP_TIME\", IntegerType()),\n",
    "    StructField(\"DEP_TIME\", IntegerType()),\n",
    "    StructField(\"WHEELS_ON\", IntegerType()),\n",
    "    StructField(\"TAXI_IN\", IntegerType()),\n",
    "    StructField(\"CRS_ARR_TIME\", IntegerType()),\n",
    "    StructField(\"ARR_TIME\", IntegerType()),\n",
    "    StructField(\"CANCELLED\", IntegerType()),\n",
    "    StructField(\"DISTANCE\", IntegerType())\n",
    "])\n",
    "\n",
    "flightTimeCsvDF = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(flightSchemaStruct) \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .option(\"dateFormat\", \"M/d/y\") \\\n",
    "    .load(\"/user/itv736079/flightdata/flight*.csv\")\n",
    "\n",
    "flightTimeCsvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68a9e2",
   "metadata": {},
   "source": [
    "### Create DataFrame from RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb4953",
   "metadata": {},
   "source": [
    "___Using `rdd.toDF()`___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066e019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "\n",
    "deptRdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c760dac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deptRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19d04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDf = deptRdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20315f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|  Finance| 10|\n",
      "|Marketing| 20|\n",
      "|    Sales| 30|\n",
      "|       IT| 40|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad11d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDf2 = deptRdd.toDF([\"dept_name\", \"dept_id\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a4e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3daa54d",
   "metadata": {},
   "source": [
    "___Using `spark.createDataframe(rdd, schema)`___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead6143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDf3 = spark.createDataFrame(deptRdd, schema = [\"dept_name\", \"dept_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97e2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDf3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960cf89c",
   "metadata": {},
   "source": [
    "### Create DataFrame from a local list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22e85eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| Name|Day|Month|Year|\n",
      "+-----+---+-----+----+\n",
      "| Ravi| 28|    1|2002|\n",
      "|Abdul| 23|    5|  81|\n",
      "| John| 12|   12|   6|\n",
      "| Rosy|  7|    8|  63|\n",
      "|Abdul| 23|    5|  81|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
    "             (\"Abdul\", \"23\", \"5\", \"81\"),\n",
    "             (\"John\", \"12\", \"12\", \"6\"),\n",
    "             (\"Rosy\", \"7\", \"8\", \"63\"),\n",
    "             (\"Abdul\", \"23\", \"5\", \"81\")\n",
    "            ]\n",
    "\n",
    "raw_df = spark.createDataFrame(data_list).toDF(\"Name\", \"Day\", \"Month\", \"Year\")\n",
    "\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8fe963",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_list = [10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38096118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "dffromList = spark.createDataFrame(normal_list, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ea48d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   10|\n",
      "|   20|\n",
      "|   30|\n",
      "|   40|\n",
      "|   50|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dffromList.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e081d8",
   "metadata": {},
   "source": [
    "### Count the number of records in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd4a4228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay,IsArrDelayed,IsDepDelayed\n",
      "1987,10,14,3,741,730,912,849,PS,1451,NA,91,79,NA,23,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n",
      "1987,10,15,4,729,730,903,849,PS,1451,NA,94,79,NA,14,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO\n",
      "1987,10,17,6,741,730,918,849,PS,1451,NA,97,79,NA,29,11,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n",
      "1987,10,18,7,729,730,847,849,PS,1451,NA,78,79,NA,-2,-1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,NO,NO\n",
      "1987,10,19,1,749,730,922,849,PS,1451,NA,93,79,NA,33,19,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n",
      "1987,10,21,3,728,730,848,849,PS,1451,NA,80,79,NA,-1,-2,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,NO,NO\n",
      "1987,10,22,4,728,730,852,849,PS,1451,NA,84,79,NA,3,-2,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,NO\n",
      "1987,10,23,5,731,730,902,849,PS,1451,NA,91,79,NA,13,1,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n",
      "1987,10,24,6,744,730,908,849,PS,1451,NA,84,79,NA,19,14,SAN,SFO,447,NA,NA,0,NA,0,NA,NA,NA,NA,NA,YES,YES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -cat /public/airtraffic_all/airtraffic/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e942f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtraffic_path = \"/public/airtraffic_all/airtraffic-part/flightmonth=200801\"\n",
    "\n",
    "airtrafficSchema = spark. \\\n",
    "    read. \\\n",
    "    parquet(airtraffic_path).\\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "266b4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "airtrafficDf = spark.read. \\\n",
    "    format(\"csv\"). \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    schema(airtrafficSchema). \\\n",
    "    option(\"path\", \"/public/airtraffic_all/airtraffic/\").\\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10afc1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235347780"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# airtrafficDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f363349b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>SPARK_PARTITION_ID()</th><th>count</th></tr>\n",
       "<tr><td>1580</td><td>648743</td></tr>\n",
       "<tr><td>1645</td><td>667411</td></tr>\n",
       "<tr><td>463</td><td>644980</td></tr>\n",
       "<tr><td>1088</td><td>664832</td></tr>\n",
       "<tr><td>833</td><td>627038</td></tr>\n",
       "<tr><td>471</td><td>667936</td></tr>\n",
       "<tr><td>1591</td><td>669356</td></tr>\n",
       "<tr><td>148</td><td>659779</td></tr>\n",
       "<tr><td>496</td><td>665848</td></tr>\n",
       "<tr><td>1238</td><td>645313</td></tr>\n",
       "<tr><td>1342</td><td>622816</td></tr>\n",
       "<tr><td>1829</td><td>648563</td></tr>\n",
       "<tr><td>540</td><td>665679</td></tr>\n",
       "<tr><td>1084</td><td>625319</td></tr>\n",
       "<tr><td>623</td><td>663435</td></tr>\n",
       "<tr><td>897</td><td>641992</td></tr>\n",
       "<tr><td>1522</td><td>664503</td></tr>\n",
       "<tr><td>1395</td><td>668808</td></tr>\n",
       "<tr><td>1507</td><td>665187</td></tr>\n",
       "<tr><td>1460</td><td>622083</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+------+\n",
       "|SPARK_PARTITION_ID()| count|\n",
       "+--------------------+------+\n",
       "|                 463|644980|\n",
       "|                 471|667936|\n",
       "|                 496|665848|\n",
       "|                 833|627038|\n",
       "|                1088|664832|\n",
       "|                1645|667411|\n",
       "|                1829|648563|\n",
       "|                1580|648743|\n",
       "|                1342|622816|\n",
       "|                 148|659779|\n",
       "|                1238|645313|\n",
       "|                1591|669356|\n",
       "|                 243|622338|\n",
       "|                 737|627061|\n",
       "|                 858|648791|\n",
       "|                1127|629597|\n",
       "|                1460|622083|\n",
       "|                1483|648222|\n",
       "|                1507|665187|\n",
       "|                1522|664503|\n",
       "+--------------------+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "# airtrafficDf. \\\n",
    "#     groupBy(spark_partition_id()). \\\n",
    "#     count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b8018",
   "metadata": {},
   "source": [
    "### How to refer a column in a Dataframe/Dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3d8f5",
   "metadata": {},
   "source": [
    "___Column string notation___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0db0fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf.select(\"order_id\", \"order_status\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5f649",
   "metadata": {},
   "source": [
    "___Column object notation___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac7793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|order_id|   order_status|\n",
      "+--------+---------------+\n",
      "|       1|         CLOSED|\n",
      "|       2|PENDING_PAYMENT|\n",
      "|       3|       COMPLETE|\n",
      "|       4|         CLOSED|\n",
      "|       5|       COMPLETE|\n",
      "+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "ordersDf.select(col(\"order_id\"), col(\"order_status\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcc4ba",
   "metadata": {},
   "source": [
    "___Column object expressions method___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fd391aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|Origin|Dest|FlightDate|\n",
      "+------+----+----------+\n",
      "|   MCI| ORD|2001-08-03|\n",
      "|   MCI| ORD|2001-08-04|\n",
      "|   MCI| ORD|2001-08-05|\n",
      "|   MCI| ORD|2001-08-06|\n",
      "|   MCI| ORD|2001-08-07|\n",
      "+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtrafficDf.select(\"Origin\", \"Dest\", \n",
    "                    to_date(\n",
    "                            concat(\"year\", \n",
    "                                   lpad(\"Month\", 2, \"0\"), \n",
    "                                   lpad(\"DayOfMonth\", 2, \"0\")\n",
    "                                   ) , 'yyyyMMdd'\n",
    "                            ).alias(\"FlightDate\")) \\\n",
    "            .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94790e2b",
   "metadata": {},
   "source": [
    "___String expression or SQL expression method___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58aaae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+\n",
      "|Origin|Dest|FlightDate|\n",
      "+------+----+----------+\n",
      "|   MCI| ORD|2001-08-03|\n",
      "|   MCI| ORD|2001-08-04|\n",
      "|   MCI| ORD|2001-08-05|\n",
      "|   MCI| ORD|2001-08-06|\n",
      "|   MCI| ORD|2001-08-07|\n",
      "+------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airtrafficDf.select(\"Origin\", \"Dest\", expr(\"\"\"to_date(\n",
    "                                                      concat(Year, \n",
    "                                                             lpad(Month, 2, 0), \n",
    "                                                             lpad(DayofMonth, 2, 0)\n",
    "                                                             ), 'yyyyMMdd') as FlightDate\"\"\")) \\\n",
    "            .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c51a7e",
   "metadata": {},
   "source": [
    "### Find duplicates in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9fd656",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [10, 20, 20, 30, 30, 30, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7cece1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "dfWithDuplData = spark.createDataFrame(data1, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e206d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   10|\n",
      "|   20|\n",
      "|   20|\n",
      "|   30|\n",
      "|   30|\n",
      "|   30|\n",
      "|   40|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDuplData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e58df158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|   20|    2|\n",
      "|   30|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicateCount = dfWithDuplData. \\\n",
    "    groupBy(\"value\"). \\\n",
    "    count(). \\\n",
    "    filter(\"count > 1\")\n",
    "\n",
    "duplicateCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "460d49f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   40|\n",
      "|   20|\n",
      "|   10|\n",
      "|   30|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithOutDup = dfWithDuplData. \\\n",
    "    dropDuplicates()\n",
    "\n",
    "dfWithOutDup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c68c440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
