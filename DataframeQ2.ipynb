{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d7f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Python - Spark Metastore'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9087f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.createDataFrame([(\"Canada\", \"100;200;300\"),\n",
    "(\"Canada\", \"400;500;600\"),\n",
    "(\"Canada\", \"700;800;900\"),\n",
    "(\"India\", \"150;250;350\"),\n",
    "(\"India\", \"450;550;650\"),\n",
    "(\"India\", \"750;850;950\"),\n",
    "(\"USA\", \"111;222;333\")], [\"Country\", \"Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5900b405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|Country|     Values|\n",
      "+-------+-----------+\n",
      "| Canada|100;200;300|\n",
      "| Canada|400;500;600|\n",
      "| Canada|700;800;900|\n",
      "|  India|150;250;350|\n",
      "|  India|450;550;650|\n",
      "|  India|750;850;950|\n",
      "|    USA|111;222;333|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9dc907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+---+---+\n",
      "|Country|     Values| c1| c2| c3|\n",
      "+-------+-----------+---+---+---+\n",
      "| Canada|100;200;300|100|200|300|\n",
      "| Canada|400;500;600|400|500|600|\n",
      "| Canada|700;800;900|700|800|900|\n",
      "|  India|150;250;350|150|250|350|\n",
      "|  India|450;550;650|450|550|650|\n",
      "|  India|750;850;950|750|850|950|\n",
      "|    USA|111;222;333|111|222|333|\n",
      "+-------+-----------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "splitted_col = split(df_raw[\"Values\"], \";\")\n",
    "\n",
    "country_1 = df_raw.withColumn(\"c1\", splitted_col.getItem(0).cast(IntegerType()))\n",
    "country_2 = country_1.withColumn(\"c2\", splitted_col.getItem(1).cast(IntegerType()))\n",
    "country_3 = country_2.withColumn(\"c3\", splitted_col.getItem(2).cast(IntegerType()))\n",
    "\n",
    "country_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcda243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|Country|sum(c1)|sum(c2)|sum(c3)|\n",
      "+-------+-------+-------+-------+\n",
      "|  India|   1350|   1650|   1950|\n",
      "|    USA|    111|    222|    333|\n",
      "| Canada|   1200|   1500|   1800|\n",
      "+-------+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_sum = country_3.groupby(\"Country\").sum()\n",
    "\n",
    "country_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7054830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|Country|         Values|\n",
      "+-------+---------------+\n",
      "|  India|1350;1650;1950;|\n",
      "|    USA|   111;222;333;|\n",
      "| Canada|1200;1500;1800;|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_res = country_sum.withColumn(\"Values\", concat(col(\"sum(c1)\"), lit(\";\"), col(\"sum(c2)\"), lit(\";\"), col(\"sum(c3)\"), lit(\";\"))). \\\n",
    "    drop(\"sum(c1)\").drop(\"sum(c2)\").drop(\"sum(c3)\")\n",
    "\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3744421",
   "metadata": {},
   "source": [
    "### The same above problem using arrayType column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc15e14",
   "metadata": {},
   "source": [
    "https://kontext.tech/column/spark/316/pyspark-convert-python-arraylist-to-spark-data-frame\n",
    "\n",
    "https://stackoverflow.com/questions/43444925/how-to-create-dataframe-from-list-in-spark-sql/50969995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37fb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
